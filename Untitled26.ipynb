{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f2ce3fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.1\n"
     ]
    }
   ],
   "source": [
    "#스킷런 포함 및 버전 확인\n",
    "import sklearn\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "79115d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#머신러닝에 필요한 데이터 및 데이터 분할, 결과출력 기능 포함\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de148165",
   "metadata": {},
   "source": [
    "# 손글씨 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e184dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#손글씨 데이터 저장\n",
    "digits_data = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2b1d6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['pixel_0_0', 'pixel_0_1', 'pixel_0_2', 'pixel_0_3', 'pixel_0_4', 'pixel_0_5', 'pixel_0_6', 'pixel_0_7', 'pixel_1_0', 'pixel_1_1', 'pixel_1_2', 'pixel_1_3', 'pixel_1_4', 'pixel_1_5', 'pixel_1_6', 'pixel_1_7', 'pixel_2_0', 'pixel_2_1', 'pixel_2_2', 'pixel_2_3', 'pixel_2_4', 'pixel_2_5', 'pixel_2_6', 'pixel_2_7', 'pixel_3_0', 'pixel_3_1', 'pixel_3_2', 'pixel_3_3', 'pixel_3_4', 'pixel_3_5', 'pixel_3_6', 'pixel_3_7', 'pixel_4_0', 'pixel_4_1', 'pixel_4_2', 'pixel_4_3', 'pixel_4_4', 'pixel_4_5', 'pixel_4_6', 'pixel_4_7', 'pixel_5_0', 'pixel_5_1', 'pixel_5_2', 'pixel_5_3', 'pixel_5_4', 'pixel_5_5', 'pixel_5_6', 'pixel_5_7', 'pixel_6_0', 'pixel_6_1', 'pixel_6_2', 'pixel_6_3', 'pixel_6_4', 'pixel_6_5', 'pixel_6_6', 'pixel_6_7', 'pixel_7_0', 'pixel_7_1', 'pixel_7_2', 'pixel_7_3', 'pixel_7_4', 'pixel_7_5', 'pixel_7_6', 'pixel_7_7']\n"
     ]
    }
   ],
   "source": [
    "#손글씨 데이터 저장이 정상적으로 되었는지 확인\n",
    "print(digits_data.keys())\n",
    "print(digits_data.target_names)\n",
    "print(digits_data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f242e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#손글씨 데이터 키값 변수에 저장\n",
    "digits_feature = digits_data.feature_names\n",
    "digits_label = digits_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4608a3da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fba221a3",
   "metadata": {},
   "source": [
    "# 일단 손글씨  데이터는 64비트 8*8로 표현되어있다.\n",
    "\n",
    "# 그리고 행렬 요소값 범위는 0~16이다.\n",
    "\n",
    "# 숫자가 0에 가까우면 검은색을 띠고 16에 가까우면 흰색을 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04571d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.  5. ...  1.  0.  0.]\n",
      "  [ 0.  0. 13. ... 15.  5.  0.]\n",
      "  [ 0.  3. 15. ... 11.  8.  0.]\n",
      "  ...\n",
      "  [ 0.  4. 11. ... 12.  7.  0.]\n",
      "  [ 0.  2. 14. ... 12.  0.  0.]\n",
      "  [ 0.  0.  6. ...  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...  5.  0.  0.]\n",
      "  [ 0.  0.  0. ...  9.  0.  0.]\n",
      "  [ 0.  0.  3. ...  6.  0.  0.]\n",
      "  ...\n",
      "  [ 0.  0.  1. ...  6.  0.  0.]\n",
      "  [ 0.  0.  1. ...  6.  0.  0.]\n",
      "  [ 0.  0.  0. ... 10.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ... 12.  0.  0.]\n",
      "  [ 0.  0.  3. ... 14.  0.  0.]\n",
      "  [ 0.  0.  8. ... 16.  0.  0.]\n",
      "  ...\n",
      "  [ 0.  9. 16. ...  0.  0.  0.]\n",
      "  [ 0.  3. 13. ... 11.  5.  0.]\n",
      "  [ 0.  0.  0. ... 16.  9.  0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.  0.  1. ...  1.  0.  0.]\n",
      "  [ 0.  0. 13. ...  2.  1.  0.]\n",
      "  [ 0.  0. 16. ... 16.  5.  0.]\n",
      "  ...\n",
      "  [ 0.  0. 16. ... 15.  0.  0.]\n",
      "  [ 0.  0. 15. ... 16.  0.  0.]\n",
      "  [ 0.  0.  2. ...  6.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  2. ...  0.  0.  0.]\n",
      "  [ 0.  0. 14. ... 15.  1.  0.]\n",
      "  [ 0.  4. 16. ... 16.  7.  0.]\n",
      "  ...\n",
      "  [ 0.  0.  0. ... 16.  2.  0.]\n",
      "  [ 0.  0.  4. ... 16.  2.  0.]\n",
      "  [ 0.  0.  5. ... 12.  0.  0.]]\n",
      "\n",
      " [[ 0.  0. 10. ...  1.  0.  0.]\n",
      "  [ 0.  2. 16. ...  1.  0.  0.]\n",
      "  [ 0.  0. 15. ... 15.  0.  0.]\n",
      "  ...\n",
      "  [ 0.  4. 16. ... 16.  6.  0.]\n",
      "  [ 0.  8. 16. ... 16.  8.  0.]\n",
      "  [ 0.  1.  8. ... 12.  1.  0.]]]\n",
      "0.0\n",
      "16.0\n"
     ]
    }
   ],
   "source": [
    "print(digits_data.images)\n",
    "print(np.min(digits_data.images))\n",
    "print(np.max(digits_data.images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ac4d2",
   "metadata": {},
   "source": [
    "# 밑에 그림은 행렬 8*8을 이미지로 표현했을때의 그림이다.\n",
    "\n",
    "# 글씨인 부분은 흰색을 가지고 있다.\n",
    "\n",
    "# 그런데 과연 흰색을 가진 부분을 모두 글씨라고 판단해도 되는 것인가?\n",
    "\n",
    "## 밑에 그림은 0이다 그런데 0이면 가운데 부분은 글씨가 아닌데 흰색을 가지고 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "609a7c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAADyUlEQVR4nO3dUVFjaRRG0T9TYyAWggSwkkgACSABL5FAJBALSCAS7higeZo6vZte6zF5+KiEXbeKB85u27YF9Pzzu38A4GvihChxQpQ4IUqcEPXvd2/udrsf+afc4/E4uvf6+jq2dblcxrZeXl7Gtm6329jWtG3bdl+97skJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEqG/PMfxUk+cR1lrrcDiMbe33+7Gtz8/Psa3T6TS2tdZa5/N5dO8rnpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IypxjuL+/H9uaPI+w1lp3d3djWx8fH2Nbb29vY1uTvx9rOccAfEOcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiMrcStnv92Nb1+t1bGut2fslk6Y/x7+NJydEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROi/spzDJfLZWzrJ5v8zm6329hWhScnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTojLnGCb/3f79/f3Y1rTJEwmTn+P5fB7bqvDkhChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQtRu27Zfv7nb/frN/9nhcJiaWu/v72Nba6319PQ0tnU8Hse2Jr+zh4eHsa1p27btvnrdkxOixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oSozK2USY+Pj6N7z8/PY1vX63Vs63Q6jW39ZG6lwB9GnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBD17TkG4Pfx5IQocUKUOCFKnBAlTogSJ0T9ByioUst9Wxj9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(digits_data.data[0].reshape(8, 8), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b70f64",
   "metadata": {},
   "source": [
    "# 먼저 필자는 글씨의 범위를 3~16으로 가정하고 이미지를 출력해보기로 했다.\n",
    "\n",
    "# 그 과정을 위해 일단 있는 데이터에서 0~2값을 0으로 하기로 했다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad82f0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  0. ...  6.  0.  0.]\n",
      " [ 0.  0.  0. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "digits_test = digits_data.data\n",
    "digits_test_1 = np.where((digits_test <= 2) == True, 0, digits_test) \n",
    "print(digits_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0660fcbc",
   "metadata": {},
   "source": [
    "# 밑의 두개의 그림중 위에 있는 그림은 글씨의 범위를 1~16\n",
    "\n",
    "# 아래에 있는 그림은 범위를 3~16으로 한 그림이다.\n",
    "\n",
    "# 아래에 있는 그림이 0에 더 가까운 그림처럼 보인다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "171cb956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAADyUlEQVR4nO3dUVFjaRRG0T9TYyAWggSwkkgACSABL5FAJBALSCAS7higeZo6vZte6zF5+KiEXbeKB85u27YF9Pzzu38A4GvihChxQpQ4IUqcEPXvd2/udrsf+afc4/E4uvf6+jq2dblcxrZeXl7Gtm6329jWtG3bdl+97skJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEqG/PMfxUk+cR1lrrcDiMbe33+7Gtz8/Psa3T6TS2tdZa5/N5dO8rnpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IypxjuL+/H9uaPI+w1lp3d3djWx8fH2Nbb29vY1uTvx9rOccAfEOcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiMrcStnv92Nb1+t1bGut2fslk6Y/x7+NJydEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROi/spzDJfLZWzrJ5v8zm6329hWhScnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTojLnGCb/3f79/f3Y1rTJEwmTn+P5fB7bqvDkhChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQtRu27Zfv7nb/frN/9nhcJiaWu/v72Nba6319PQ0tnU8Hse2Jr+zh4eHsa1p27btvnrdkxOixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oSozK2USY+Pj6N7z8/PY1vX63Vs63Q6jW39ZG6lwB9GnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBD17TkG4Pfx5IQocUKUOCFKnBAlTogSJ0T9ByioUst9Wxj9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAADqklEQVR4nO3dwVEiURhG0e4pEyAFUsBUMAVSMARyIQRJgVQ0hDYBdDc/d5hzlvTi60JvvSoXvnXbtgXo+fPoFwDuEydEiROixAlR4oSol98eruv6lH/KPR6Po3vn83ls63q9jm2dTqexrWe2bdt673MnJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6J+vY7hWU1ej7Asy7Lf78e2drvd2Nbkxctvb29jW8uyLJfLZXTvHicnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTojLXMRwOh7GtyesRlmVZ1nUd3Zvy8fExtjX5+7EsrmMAfiFOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRGXuStntdmNbt9ttbOuZ+R7/LicnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTov7L6xiu1+vY1jOb/Jl9fX2NbVU4OSFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBCVuY5h8t/tHw6Hsa1nNvk9Xi6Xsa0KJydEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROi1m3bfn64rj8//Id9fn6O7p1Op7Gt4/E4trXf78e2Xl9fx7ambdu23vvcyQlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocULUy6Nf4BHe399H987n89jW7XYb23rm+0sKnJwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IWrdte/Q7AHc4OSFKnBAlTogSJ0SJE6LECVHfZ39DOq3JDLwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(digits_data.data[0].reshape(8, 8), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.imshow(digits_test_1[0].reshape(8, 8), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c0eb0b",
   "metadata": {},
   "source": [
    "# 밑의 그림중 위의 0~9는 글씨의 범위를 1~16\n",
    "\n",
    "# 밑의 0~9는 3~16까지 글씨라고 판단한 그림이다.\n",
    "\n",
    "## 두 그림을 비교해보면 범위를 줄인 그림은 글씨의 범위는 줄었지만\n",
    "\n",
    "## 더 정확한 모양을 표시하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29179d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(digits_data.data[i].reshape(8, 8), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(digits_test_1[i].reshape(8, 8), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0bc9bf",
   "metadata": {},
   "source": [
    "# 위의 테스트로 글씨의 정확성이 올라간것을 확인했다.\n",
    "\n",
    "# 더 정확한 확인을 위해 글씨의 범위를 1씩 줄여서 글씨를 확인해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1faed8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAC+CAYAAACWL9wvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAHUElEQVR4nO3dMVIUaxcG4G/+urlcNyClC4AqzSExZiJTyQhhKRCaSaoJxBJgLlW4ACzdgMWsgLuAv/q0fkO/NPo86XF62qbnrQ7eOr24u7trAGT876FPAOBvInQBgoQuQJDQBQgSugBB/1TDxWIxSbXh5uZmcHZxcTE4Ozg4mOJ02t3d3eJX/+1U1+Ty8nJwtrGxMTjb3t6+93Np7feuSWvTXZfK7e3t4Oz6+npwtru72/2dc7hXqsbRt2/fBmcvXryY4nRmcU0qZ2dng7PlcjnJd1bXxJMuQJDQBQgSugBBQhcgSOgCBAldgKCyMtarqoS11trz588HZ//+++/grKrKLBa/1XCanar+tLOzMzg7Pj4enB0dHfWf0ExUf/PVajU429zcnOBsMsZ+P9W9boHV/5uqFtbLky5AkNAFCBK6AEFCFyBI6AIECV2AoMVIDaurfzJWW+mtd3369Glw9vr1665jtjaPLUlVZezJkyeDs6mqcnPZMlZtC6s2rE1VL5zDvTJWKRvyJ28Z671PpmLLGMBMCF2AIKELECR0AYKELkCQ0AUIEroAQZOsdry6uprisGt1cR9a1cNtre7i/s2qjmV1Tc/Pz+//ZGaiWo1avU37T9a7yvMhVqN60gUIEroAQUIXIEjoAgQJXYAgoQsQNMlqx6m8e/ducHZwcNB93Dmsput9i+vfvNqxqgltbGzc+7m0No97pfLly5euz7169ar7OxPXZOz3cXp6Ojjb39/vOu5UK0A96QIECV2AIKELECR0AYKELkCQ0AUImmTLWPXW3tb6t4W9fPmy63M8rGqT0+HhYfdxp6rLPWZV9evDhw/BM7lfq9WqnL99+3Zw9hBvA6540gUIEroAQUIXIEjoAgQJXYAgoQsQNMmWsZ8/f5bzp0+fDs6qWkv1Qr65b0n6hXPo+txj3zJWbRFrrbWtra2ew5Yvplwul13HbG0e90qv6nf58ePH8rPVFr+5X5PqBaZnZ2eDs2o72RhbxgBmQugCBAldgCChCxAkdAGChC5AkNAFCHqQtwHf3NwMzq6urgZnb968meJ0ZtEzrPqCe3t7g7Pet6COmcvbgCu91+zk5KQ87tHR0eBsDvdK9fupXFxcDM4e+9u0K1O98XfkO/V0AeZA6AIECV2AIKELECR0AYKELkBQWRkD4H550gUIEroAQUIXIEjoAgQJXYAgoQsQJHQBgoQuQJDQBQgSugBBQhcgSOgCBAldgCChCxAkdAGChC5AkNAFCBK6AEFCFyBI6AIECV2AIKELECR0AYKELkCQ0AUIEroAQUIXIEjoAgQJXYAgoQsQJHQBgoQuQJDQBQgSugBBQhcgSOgCBAldgCChCxAkdAGChC5AkNAFCBK6AEFCFyBI6AIECV2AIKELECR0AYKELkCQ0AUIEroAQUIXIEjoAgQJXYAgoQsQJHQBgoQuQJDQBQgSugBBQhcgSOgCBP1TDReLxV3PQS8vL8v59+/fB2f7+/s9X7mWu7u7xa/+295rMqa6ZhsbG4Oz7e3tez+X1n7vmrQ23XWpXF9fD862trYGZ6vVqjxudb0T98rx8XE5Xy6Xg7PNzc2er1xL4pqcnZ2V8+pvtru72/OVa6muiSddgCChCxAkdAGChC5AkNAFCBK6AEGLu7vhBkdvvaOqhLXW2rNnz3oO2378+DE4W6cqM4fK2MjfYYqvLD2GylilqhhV9aLW6opR4l4Zq1z23uvV73KdWlXimlS/j3V8/fp1cLZOHVNlDGAmhC5AkNAFCBK6AEFCFyBI6AIElVvGet3e3pbzqjJWbYAaq9LMWbURi/tX1aqm2sx2X8buld4tfWO/y8fs8+fPg7OpqnK9POkCBAldgCChCxAkdAGChC5AkNAFCBK6AEGTrHYce3Pn3t5ez2EnW3GYWE03tu6y6lA+RK/0Max27O2djq12rMx9DWjVV626y3NfjTq22rHKhqrfX/22prpPPOkCBAldgCChCxAkdAGChC5AkNAFCJpkteNyuZzisLN7Y+7vGKufjFXK/lRj9cLe1Xvr1H0es52dncHZ3H8jleqtva3V2XBycjI4e4g6piddgCChCxAkdAGChC5AkNAFCBK6AEGTbBmbSm8tpLXWjo6OquNOviVp7A2vW1tbg7OqLlN9bp2K0Fy2jM2tJjj3jVrv378fnFX1qHWqU3PYvNbr+Ph4cDa2ea2qxtoyBjATQhcgSOgCBAldgCChCxAkdAGCJtkyto7qJXKVqhI2B2OVnKomVG0gW+eFgnNQVXZaa221WmVOZEbW+T/v7+8PzsaqaH+q6h6rrtdUm+o86QIECV2AIKELECR0AYKELkCQ0AUIEroAQZP0dMe6l4eHh13HPT097frcY1Ctpqy6hL295rkYe9vv3/hW37G/adW3rTq+5+fnvaf04MYyperBV/fQQ9xfnnQBgoQuQJDQBQgSugBBQhcgSOgCBJVvAwbgfnnSBQgSugBBQhcgSOgCBAldgCChCxD0H8OBPfiKD3w+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAC+CAYAAACWL9wvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAE/ElEQVR4nO3dQVLqQBSGUfLqzaHcgOx/VboBi6wgb/JmSkcC/feFnDONSmiTrzK41ZmWZTkAkPFn9AkA7InoAgSJLkCQ6AIEiS5A0N/WwWmadjHasCzL9NuftSY/G7Eul8vl6rHT6dTlMytcK62Jo2m66d/2EBXWpJrWmnjSBQgSXYAg0QUIEl2AINEFCBJdgKBpZfyk1MhLr1GZ6iMvI0aEqoyMtb77PM9Xj7XGyc7n8z3n41r5/pml12QEI2MARYguQJDoAgSJLkCQ6AIEiS5AUHOXsa3W3ru2dXRlxA5KKa0Rpz3b+j/f67v/XvkeeRWedAGCRBcgSHQBgkQXIEh0AYJEFyBIdAGCuszpmhX8bm0O93g8Zk7khbTW9JWvwWpvA65gxJuht/KkCxAkugBBogsQJLoAQaILECS6AEFdRsb4bm1sZa9bEXK7PY6F9doudsT4nSddgCDRBQgSXYAg0QUIEl2AINEFCDIyxlD3jMrtcXRqr+Z5bh5/pp3XPOkCBIkuQJDoAgSJLkCQ6AIEiS5AUJeRsa+vr+bxt7e3Hh/LE1ob52mNAj3TmFAFrfuy+j15z8slq73A1JMuQJDoAgSJLkCQ6AIEiS5AkOgCBIkuQNC0Muu4i1fULsvy62G9EWsyYh71ljX5fx6lrpVea1b9Whmh+ppUu3886QIEiS5AkOgCBIkuQJDoAgSJLkBQc2QMgMfypAsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBogsQJLoAQaILECS6AEGiCxAkugBBf1sHp2laUicy0rIs029/1pr8rNq6LMv105nnufm7p9Op9XeHXysfHx9Xj53P5x4f2VRhTapprYknXYAg0QUIEl2AINEFCBJdgCDRBQhqjoxt1RppORwOh/f3901/9/Pz8+qxEaMyj9QacZqmm6a3OOx3zaqNkz1K6/64x4jrxJMuQJDoAgSJLkCQ6AIEiS5AkOgCBE0ro0pd5jS27gDV2v3pHhV2Sao2Mvbsu4z1UuFa2epyuVw9ds+9lViTtZGxrfdIrxE7u4wBFCG6AEGiCxAkugBBogsQJLoAQaILENRla8d7HI/H0afQxdp2l3vdivAerbnTll7z3ilbZ7q3rtcrG3EteNIFCBJdgCDRBQgSXYAg0QUIEl2AoCEjY1vHo6ptf3iLZx9TGmWvY2FbPfM90rJ27s/0vT3pAgSJLkCQ6AIEiS5AkOgCBIkuQNCQtwFv1WsspMIbXrd+twpr8v+zSq1LL8/85tteKtw/1XgbMEARogsQJLoAQaILECS6AEGiCxBU7sWU3Gae59Gn0NWrf7+f9PrO1cbvKmjtYtdrpzpPugBBogsQJLoAQaILECS6AEGiCxAkugBBQ7Z2XNu67ppes4TVt6YbMUtYZWvHaipcK637pzXjW+FacZ140gWIEl2AINEFCBJdgCDRBQgSXYCg5sgYAI/lSRcgSHQBgkQXIEh0AYJEFyBIdAGC/gH+GGqzReFKHAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAC+CAYAAACWL9wvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEg0lEQVR4nO3dwXKjRhRA0SE1e+n/v9J8Adkki8R2M5bNpZHO2cozhja61YtXzbJt2y8AGn+dfQEAr0R0AUKiCxASXYCQ6AKEfo8+XJblJUYbtm1b/vRnrcnHzliXt7e3Tz+73++H/M4ZnpXRxNGyfOnP9iNmWJPZjNbEThcgJLoAIdEFCIkuQEh0AUKiCxAajowd5dGRl9lGZSqvet+/fo3vfV3X8Erm4TtybXa6ACHRBQiJLkBIdAFCogsQEl2A0CEjY3vvXXt0dOWZR15GJ2a9skf/5q86OvXM9/Ys7HQBQqILEBJdgJDoAoREFyAkugAh0QUIHTKna1bwvb053Nvt1lzIExmt6TM/g686gzxyxpuhH2WnCxASXYCQ6AKERBcgJLoAIdEFCJ3yNuBXtDe2snccJvzrFcfCjjou9ozxOztdgJDoAoREFyAkugAh0QUIiS5AyMgYp/rOqNwrjk69qnVdh59f6eQ1O12AkOgChEQXICS6ACHRBQiJLkDIyBin2hvnGY0CXWlMiO/5zsslZ3uBqZ0uQEh0AUKiCxASXYCQ6AKERBcgJLoAIXO6kxjNC77ym4Kv9JZX5nS73c6+hP+w0wUIiS5ASHQBQqILEBJdgJDoAoSWVx5HAqjZ6QKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCP0efbgsy1ZdyJm2bVv+9GetycdmW5dt+/xy1nUd/tv7/T76fz0r/2NN3hutiZ0uQEh0AUKiCxASXYCQ6AKERBcgNBwZO8ponGdkWb40xXQpozV55vs+ijV7Lo82Y88Zz4mdLkBIdAFCogsQEl2AkOgChEQXIHTKyNjI3glQwGPe3t4+/Wx0stoVXGlE0E4XICS6ACHRBQiJLkBIdAFCogsQEl2A0LJzpOAh56nNdrSjt5m+d4W3AY/mTke+M5M6w7My2zGgxZrsNePR+z5qdtnbgAEmIboAIdEFCIkuQEh0AUKiCxA65WjHR8c7ZhuV+Yq98aarH613lDPGwq7syt+Rkb1rv9J92+kChEQXICS6ACHRBQiJLkBIdAFCp5wy9qijxkKufHLUDGvyz++aal2OcuUTtY4yw/dnNk4ZA5iE6AKERBcgJLoAIdEFCIkuQOiUU8b4Oeu6nn0Jh3r2+/vIUfc82/jdDI56MeWInS5ASHQBQqILEBJdgJDoAoREFyAkugChU4523Du67jNHzRLOfjTdGbOEsxztOJsZnpXR92c04zvDs+I5sdMFSIkuQEh0AUKiCxASXYCQ6AKEhiNjAPwsO12AkOgChEQXICS6ACHRBQiJLkDobx2yIZv6F4+VAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAC+CAYAAACWL9wvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAADw0lEQVR4nO3dwU7jQBQAwcyK//9l750EhzikPbarriCUGUzrHZ7MWJblBkDj394fAOBKRBcgJLoAIdEFCIkuQOhr7YtjjEusNizLMn77ve7kMfdyz53ccycmXYCU6AKERBcgJLoAIdEFCIkuQGh1ZYw5rL2UaIyXNrsO58pn55xMugAh0QUIiS5ASHQBQqILEBJdgJCVMaa2dS3MqhmzMukChEQXICS6ACHRBQiJLkBIdAFCogsQsqc7ibW9Ul5nF5dZmXQBQqILEBJdgJDoAoREFyAkugAhK2PA9J6tVB7pFaAmXYCQ6AKERBcgJLoAIdEFCIkuQMjKGLt65+1q3iR2Hc9+10f6788mXYCQ6AKERBcgJLoAIdEFCIkuQMjKGLt6ZxXoSGtCfNbW3/cez4lJFyAkugAh0QUIiS5ASHQBQqILEBJdgJA93Ums7Qu+8/rDozvSf3llTrM9CyZdgJDoAoREFyAkugAh0QUIiS5AaFx5HQmgZtIFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxD6WvviGGOpPsielmUZv/1ed/LYbPeyLD9/nDFeOtr3n+tZ+cad3Fu7E5MuQEh0AUKiCxASXYCQ6AKERBcgtLoy9ilr6zxr3ln1md2nVpyuyp2dy9ZmPLPHc2LSBQiJLkBIdAFCogsQEl2AkOgChHZZGVtj1Qd41ZG6YdIFCIkuQEh0AUKiCxASXYCQ6AKERBcgNN2e7lVfcXjms/G3rvo3chYmXYCQ6AKERBcgJLoAIdEFCIkuQGiXlbGtay1WZWDdWf9Gnn32I53bpAsQEl2AkOgChEQXICS6ACHRBQhN95axq9q68nKkVZktzn6+R9bOfLud99zvONKdmHQBQqILEBJdgJDoAoREFyAkugAhK2MHd6RVmS3Ofr5HPnXmK67fzcikCxASXYCQ6AKERBcgJLoAIdEFCIkuQGiXPd1nr677yZl3Cc98Nv7W1n1bz9gcTLoAIdEFCIkuQEh0AUKiCxASXYDQ2Lq+BcDrTLoAIdEFCIkuQEh0AUKiCxASXYDQfwmeicSL6sChAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 1797, 64)\n"
     ]
    }
   ],
   "source": [
    "digits_test_arr = np.empty((0,1797,64), int)\n",
    "for i in range(15):\n",
    "    digits_test_t = np.where((digits_test <= i+1) == True, 0, digits_test)\n",
    "    digits_test_arr = np.append(digits_test_arr, [digits_test_t], axis = 0)\n",
    "    for i in range(10):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.imshow(digits_test_t[i].reshape(8, 8), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(digits_test_arr.shape)\n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb74923",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81be5a3a",
   "metadata": {},
   "source": [
    "### 현재 가정으로 전처리를 한 데이터로 학습 데이터와 테스트 데이터를 나누고\n",
    "\n",
    "### 의사 결정 트리를 통해 정확도를 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "96d8a654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96        37\n",
      "           1       0.76      0.81      0.78        31\n",
      "           2       0.81      0.89      0.85        28\n",
      "           3       0.90      0.92      0.91        38\n",
      "           4       0.80      0.78      0.79        41\n",
      "           5       0.87      0.82      0.84        33\n",
      "           6       0.90      0.84      0.87        45\n",
      "           7       0.90      0.88      0.89        40\n",
      "           8       0.82      0.80      0.81        35\n",
      "           9       0.76      0.78      0.77        32\n",
      "\n",
      "    accuracy                           0.85       360\n",
      "   macro avg       0.85      0.85      0.85       360\n",
      "weighted avg       0.85      0.85      0.85       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        37\n",
      "           1       0.76      0.71      0.73        31\n",
      "           2       0.69      0.79      0.73        28\n",
      "           3       0.84      0.84      0.84        38\n",
      "           4       0.81      0.85      0.83        41\n",
      "           5       0.83      0.88      0.85        33\n",
      "           6       0.84      0.82      0.83        45\n",
      "           7       0.94      0.80      0.86        40\n",
      "           8       0.75      0.69      0.72        35\n",
      "           9       0.89      1.00      0.94        32\n",
      "\n",
      "    accuracy                           0.84       360\n",
      "   macro avg       0.84      0.84      0.83       360\n",
      "weighted avg       0.84      0.84      0.84       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94        37\n",
      "           1       0.79      0.87      0.83        31\n",
      "           2       0.92      0.86      0.89        28\n",
      "           3       0.85      0.87      0.86        38\n",
      "           4       0.93      0.93      0.93        41\n",
      "           5       0.90      0.85      0.88        33\n",
      "           6       0.91      0.93      0.92        45\n",
      "           7       0.95      0.88      0.91        40\n",
      "           8       0.87      0.94      0.90        35\n",
      "           9       0.83      0.91      0.87        32\n",
      "\n",
      "    accuracy                           0.89       360\n",
      "   macro avg       0.89      0.89      0.89       360\n",
      "weighted avg       0.90      0.89      0.90       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99        37\n",
      "           1       0.77      0.77      0.77        31\n",
      "           2       0.88      0.79      0.83        28\n",
      "           3       0.87      0.89      0.88        38\n",
      "           4       0.84      0.88      0.86        41\n",
      "           5       0.90      0.85      0.88        33\n",
      "           6       0.84      0.84      0.84        45\n",
      "           7       0.84      0.90      0.87        40\n",
      "           8       0.84      0.77      0.81        35\n",
      "           9       0.85      0.88      0.86        32\n",
      "\n",
      "    accuracy                           0.86       360\n",
      "   macro avg       0.86      0.86      0.86       360\n",
      "weighted avg       0.86      0.86      0.86       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97        37\n",
      "           1       0.79      0.84      0.81        31\n",
      "           2       0.92      0.82      0.87        28\n",
      "           3       0.81      0.89      0.85        38\n",
      "           4       0.85      0.80      0.83        41\n",
      "           5       0.83      0.88      0.85        33\n",
      "           6       0.97      0.87      0.92        45\n",
      "           7       0.93      0.93      0.93        40\n",
      "           8       0.78      0.83      0.81        35\n",
      "           9       0.74      0.78      0.76        32\n",
      "\n",
      "    accuracy                           0.86       360\n",
      "   macro avg       0.86      0.86      0.86       360\n",
      "weighted avg       0.87      0.86      0.86       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        37\n",
      "           1       0.74      0.94      0.83        31\n",
      "           2       0.91      0.75      0.82        28\n",
      "           3       0.94      0.84      0.89        38\n",
      "           4       0.88      0.85      0.86        41\n",
      "           5       0.84      0.82      0.83        33\n",
      "           6       0.95      0.84      0.89        45\n",
      "           7       0.83      0.85      0.84        40\n",
      "           8       0.58      0.71      0.64        35\n",
      "           9       0.79      0.72      0.75        32\n",
      "\n",
      "    accuracy                           0.84       360\n",
      "   macro avg       0.84      0.83      0.83       360\n",
      "weighted avg       0.85      0.84      0.84       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96        37\n",
      "           1       0.84      0.84      0.84        31\n",
      "           2       0.83      0.89      0.86        28\n",
      "           3       0.78      0.92      0.84        38\n",
      "           4       0.92      0.80      0.86        41\n",
      "           5       0.85      0.85      0.85        33\n",
      "           6       0.86      0.80      0.83        45\n",
      "           7       0.84      0.80      0.82        40\n",
      "           8       0.66      0.77      0.71        35\n",
      "           9       0.88      0.72      0.79        32\n",
      "\n",
      "    accuracy                           0.84       360\n",
      "   macro avg       0.84      0.84      0.84       360\n",
      "weighted avg       0.84      0.84      0.84       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        37\n",
      "           1       0.78      0.90      0.84        31\n",
      "           2       0.80      0.86      0.83        28\n",
      "           3       0.85      0.92      0.89        38\n",
      "           4       1.00      0.88      0.94        41\n",
      "           5       0.81      0.88      0.84        33\n",
      "           6       0.89      0.87      0.88        45\n",
      "           7       0.85      0.82      0.84        40\n",
      "           8       0.76      0.71      0.74        35\n",
      "           9       0.88      0.72      0.79        32\n",
      "\n",
      "    accuracy                           0.86       360\n",
      "   macro avg       0.86      0.86      0.85       360\n",
      "weighted avg       0.86      0.86      0.86       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.95        37\n",
      "           1       0.71      0.87      0.78        31\n",
      "           2       0.85      0.79      0.81        28\n",
      "           3       0.88      0.95      0.91        38\n",
      "           4       0.97      0.88      0.92        41\n",
      "           5       0.84      0.94      0.89        33\n",
      "           6       0.90      0.78      0.83        45\n",
      "           7       0.94      0.85      0.89        40\n",
      "           8       0.76      0.83      0.79        35\n",
      "           9       0.86      0.78      0.82        32\n",
      "\n",
      "    accuracy                           0.86       360\n",
      "   macro avg       0.86      0.86      0.86       360\n",
      "weighted avg       0.87      0.86      0.86       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85        37\n",
      "           1       0.85      0.94      0.89        31\n",
      "           2       0.84      0.75      0.79        28\n",
      "           3       0.84      0.84      0.84        38\n",
      "           4       0.89      0.78      0.83        41\n",
      "           5       0.71      0.67      0.69        33\n",
      "           6       0.82      0.73      0.78        45\n",
      "           7       0.91      0.78      0.84        40\n",
      "           8       0.66      0.83      0.73        35\n",
      "           9       0.71      0.78      0.75        32\n",
      "\n",
      "    accuracy                           0.80       360\n",
      "   macro avg       0.80      0.80      0.80       360\n",
      "weighted avg       0.81      0.80      0.80       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.92      0.80        37\n",
      "           1       0.74      0.81      0.77        31\n",
      "           2       0.81      0.79      0.80        28\n",
      "           3       0.89      0.89      0.89        38\n",
      "           4       0.85      0.80      0.83        41\n",
      "           5       0.76      0.79      0.78        33\n",
      "           6       0.84      0.71      0.77        45\n",
      "           7       0.94      0.78      0.85        40\n",
      "           8       0.53      0.60      0.56        35\n",
      "           9       0.76      0.69      0.72        32\n",
      "\n",
      "    accuracy                           0.78       360\n",
      "   macro avg       0.78      0.78      0.78       360\n",
      "weighted avg       0.79      0.78      0.78       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.68      0.63        37\n",
      "           1       0.70      0.68      0.69        31\n",
      "           2       0.61      0.68      0.64        28\n",
      "           3       0.81      0.92      0.86        38\n",
      "           4       0.72      0.76      0.74        41\n",
      "           5       0.71      0.76      0.74        33\n",
      "           6       0.87      0.73      0.80        45\n",
      "           7       0.85      0.70      0.77        40\n",
      "           8       0.57      0.57      0.57        35\n",
      "           9       0.73      0.69      0.71        32\n",
      "\n",
      "    accuracy                           0.72       360\n",
      "   macro avg       0.72      0.72      0.71       360\n",
      "weighted avg       0.73      0.72      0.72       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.64        37\n",
      "           1       0.76      0.84      0.80        31\n",
      "           2       0.68      0.82      0.74        28\n",
      "           3       0.76      0.89      0.82        38\n",
      "           4       0.81      0.71      0.75        41\n",
      "           5       0.72      0.64      0.68        33\n",
      "           6       0.91      0.71      0.80        45\n",
      "           7       0.78      0.72      0.75        40\n",
      "           8       0.57      0.49      0.52        35\n",
      "           9       0.59      0.59      0.59        32\n",
      "\n",
      "    accuracy                           0.71       360\n",
      "   macro avg       0.71      0.71      0.71       360\n",
      "weighted avg       0.72      0.71      0.71       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73        37\n",
      "           1       0.72      0.68      0.70        31\n",
      "           2       0.64      0.89      0.75        28\n",
      "           3       0.76      0.82      0.78        38\n",
      "           4       0.74      0.63      0.68        41\n",
      "           5       0.62      0.61      0.62        33\n",
      "           6       0.74      0.69      0.71        45\n",
      "           7       0.70      0.65      0.68        40\n",
      "           8       0.46      0.34      0.39        35\n",
      "           9       0.59      0.69      0.64        32\n",
      "\n",
      "    accuracy                           0.68       360\n",
      "   macro avg       0.67      0.68      0.67       360\n",
      "weighted avg       0.67      0.68      0.67       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.54      0.53        37\n",
      "           1       0.86      0.77      0.81        31\n",
      "           2       0.50      0.68      0.58        28\n",
      "           3       0.58      0.74      0.65        38\n",
      "           4       0.67      0.68      0.67        41\n",
      "           5       0.50      0.55      0.52        33\n",
      "           6       0.78      0.56      0.65        45\n",
      "           7       0.62      0.65      0.63        40\n",
      "           8       0.37      0.29      0.32        35\n",
      "           9       0.61      0.53      0.57        32\n",
      "\n",
      "    accuracy                           0.60       360\n",
      "   macro avg       0.60      0.60      0.59       360\n",
      "weighted avg       0.61      0.60      0.60       360\n",
      "\n",
      "0.8944444444444445 2 0.8949385396443713 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=20)\n",
    "max = 0\n",
    "pre_max = 0\n",
    "loc = 0\n",
    "loc_pre = 0\n",
    "for i in range(15):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(digits_test_arr[i],\n",
    "                                                        digits_label, \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=20)\n",
    "   \n",
    "    decision_tree.fit(X_train, y_train)\n",
    "    y_pred = decision_tree.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    pre = precision_score(y_test, y_pred, average = 'macro')\n",
    "    if max < accuracy:\n",
    "        max = accuracy\n",
    "        loc = i\n",
    "    if pre_max < pre:\n",
    "        pre_max = pre\n",
    "        loc_pre = i\n",
    "        \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "print(max, loc, pre_max, loc_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e7c72e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8505880845218841\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91        32\n",
      "           1       0.96      0.72      0.83        36\n",
      "           2       0.96      0.73      0.83        30\n",
      "           3       0.72      0.80      0.76        41\n",
      "           4       0.89      0.78      0.83        32\n",
      "           5       0.88      0.93      0.91        46\n",
      "           6       0.86      0.94      0.90        32\n",
      "           7       0.97      0.93      0.95        40\n",
      "           8       0.73      0.83      0.78        42\n",
      "           9       0.66      0.72      0.69        29\n",
      "\n",
      "    accuracy                           0.84       360\n",
      "   macro avg       0.85      0.83      0.84       360\n",
      "weighted avg       0.85      0.84      0.84       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(digits_data.data,\n",
    "                                                        digits_label, \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=21)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "a = precision_score(y_test, y_pred, average = 'macro')\n",
    "print(a)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98670e4b",
   "metadata": {},
   "source": [
    "### 위의 코드로 알 수 있는 점은 데이터와 모델중 간단한 데이터 전처리를 해주었지만\n",
    "\n",
    "### 그 결과는 더 높은 정확도와  Precision이 나왔다\n",
    "\n",
    "### 그러면 모델도 영향을 주는지 Random Forest, SVM, Logistic Regression을 써보기로 했다.\n",
    "\n",
    "### 참고로 먼저 데이터는 그대로 두고 모델만 바꿔서 값을 본다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da53f865",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d5ff13a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        30\n",
      "           1       0.97      0.97      0.97        40\n",
      "           2       1.00      1.00      1.00        38\n",
      "           3       1.00      1.00      1.00        29\n",
      "           4       0.97      0.95      0.96        40\n",
      "           5       0.96      0.98      0.97        44\n",
      "           6       1.00      1.00      1.00        31\n",
      "           7       0.93      1.00      0.97        28\n",
      "           8       1.00      0.95      0.97        38\n",
      "           9       1.00      1.00      1.00        42\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits_data.data, \n",
    "                                                    digits_label, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=23)\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=23)\n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0249ffb",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c730c839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        31\n",
      "           1       0.95      1.00      0.97        38\n",
      "           2       1.00      1.00      1.00        38\n",
      "           3       0.96      0.96      0.96        27\n",
      "           4       0.98      0.98      0.98        41\n",
      "           5       1.00      1.00      1.00        35\n",
      "           6       1.00      1.00      1.00        38\n",
      "           7       1.00      1.00      1.00        34\n",
      "           8       0.97      0.94      0.96        35\n",
      "           9       0.98      0.98      0.98        43\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm_model = svm.SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_svm_pred = svm_model.predict(X_test)\n",
    "print(classification_report(y_test, y_svm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ef46e1",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "88cea7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        31\n",
      "           1       0.93      1.00      0.96        38\n",
      "           2       0.97      1.00      0.99        38\n",
      "           3       0.93      0.93      0.93        27\n",
      "           4       0.95      0.98      0.96        41\n",
      "           5       0.95      1.00      0.97        35\n",
      "           6       1.00      0.97      0.99        38\n",
      "           7       0.94      1.00      0.97        34\n",
      "           8       1.00      0.91      0.96        35\n",
      "           9       0.97      0.91      0.94        43\n",
      "\n",
      "    accuracy                           0.96       360\n",
      "   macro avg       0.96      0.96      0.96       360\n",
      "weighted avg       0.97      0.96      0.96       360\n",
      "\n",
      "0.974667800752832\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_model = SGDClassifier()\n",
    "sgd_model.fit(X_train, y_train)\n",
    "sgd_pred = sgd_model.predict(X_test)\n",
    "print(classification_report(y_test, sgd_pred))\n",
    "pre = precision_score(y_test, y_pred, average = 'macro')\n",
    "print(pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a612ae",
   "metadata": {},
   "source": [
    "### 의사결정 트리를 제외한 나머지 모델들을 돌린 결과 다른 모델들이 더 좋은 결과를 주었다.\n",
    "\n",
    "### 자 그러면 이제 데이터와 모델을 서로 바꾸면 어떤 결과가 나올까?\n",
    "\n",
    "# 참고로 각 모델마다 어떤 데이터에서 좋은 결과를 내는지 보기 위해\n",
    "\n",
    "# 맨 밑줄에 accuracy 값, 위치, precision 값, 위치를 출력했다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d91dd7",
   "metadata": {},
   "source": [
    "# RandomForest 데이터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a6c3cdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        32\n",
      "           1       1.00      1.00      1.00        40\n",
      "           2       0.97      1.00      0.99        33\n",
      "           3       0.95      0.95      0.95        39\n",
      "           4       1.00      0.98      0.99        44\n",
      "           5       1.00      1.00      1.00        36\n",
      "           6       1.00      1.00      1.00        31\n",
      "           7       0.94      1.00      0.97        30\n",
      "           8       1.00      0.97      0.99        40\n",
      "           9       0.97      0.94      0.96        35\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        31\n",
      "           1       1.00      0.97      0.99        38\n",
      "           2       1.00      1.00      1.00        38\n",
      "           3       0.93      0.93      0.93        27\n",
      "           4       0.97      0.95      0.96        41\n",
      "           5       0.97      1.00      0.99        35\n",
      "           6       1.00      1.00      1.00        38\n",
      "           7       0.92      1.00      0.96        34\n",
      "           8       0.97      0.94      0.96        35\n",
      "           9       0.98      0.98      0.98        43\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.97      0.97      0.97       360\n",
      "weighted avg       0.98      0.97      0.98       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        36\n",
      "           1       1.00      1.00      1.00        36\n",
      "           2       1.00      1.00      1.00        33\n",
      "           3       1.00      0.95      0.97        40\n",
      "           4       1.00      1.00      1.00        40\n",
      "           5       0.97      0.97      0.97        37\n",
      "           6       1.00      0.94      0.97        35\n",
      "           7       0.90      0.97      0.94        38\n",
      "           8       0.94      0.94      0.94        31\n",
      "           9       0.91      0.94      0.93        34\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.97      0.97      0.97       360\n",
      "weighted avg       0.97      0.97      0.97       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        31\n",
      "           1       1.00      0.97      0.99        38\n",
      "           2       1.00      1.00      1.00        38\n",
      "           3       0.96      0.96      0.96        27\n",
      "           4       0.98      0.98      0.98        41\n",
      "           5       0.97      1.00      0.99        35\n",
      "           6       1.00      1.00      1.00        38\n",
      "           7       0.97      1.00      0.99        34\n",
      "           8       0.97      0.97      0.97        35\n",
      "           9       0.98      0.98      0.98        43\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        30\n",
      "           1       0.95      1.00      0.97        37\n",
      "           2       0.97      1.00      0.99        38\n",
      "           3       1.00      0.95      0.97        37\n",
      "           4       1.00      1.00      1.00        38\n",
      "           5       1.00      0.98      0.99        45\n",
      "           6       1.00      1.00      1.00        42\n",
      "           7       1.00      0.96      0.98        26\n",
      "           8       1.00      0.91      0.95        34\n",
      "           9       0.89      0.97      0.93        33\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        37\n",
      "           1       0.94      0.97      0.96        34\n",
      "           2       0.97      1.00      0.99        34\n",
      "           3       0.97      0.97      0.97        40\n",
      "           4       0.97      0.97      0.97        34\n",
      "           5       0.91      1.00      0.96        32\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      1.00      1.00        40\n",
      "           8       0.91      0.91      0.91        33\n",
      "           9       1.00      0.90      0.95        39\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.97      0.97      0.97       360\n",
      "weighted avg       0.97      0.97      0.97       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        31\n",
      "           1       0.95      0.97      0.96        39\n",
      "           2       1.00      1.00      1.00        43\n",
      "           3       0.97      0.95      0.96        40\n",
      "           4       1.00      0.94      0.97        36\n",
      "           5       0.86      1.00      0.93        25\n",
      "           6       1.00      1.00      1.00        34\n",
      "           7       0.90      0.97      0.94        37\n",
      "           8       1.00      0.86      0.92        43\n",
      "           9       0.94      0.97      0.95        32\n",
      "\n",
      "    accuracy                           0.96       360\n",
      "   macro avg       0.96      0.97      0.96       360\n",
      "weighted avg       0.97      0.96      0.96       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        32\n",
      "           1       0.95      1.00      0.97        36\n",
      "           2       1.00      0.97      0.99        38\n",
      "           3       0.98      0.98      0.98        45\n",
      "           4       1.00      0.97      0.99        37\n",
      "           5       0.92      1.00      0.96        33\n",
      "           6       1.00      1.00      1.00        43\n",
      "           7       0.96      1.00      0.98        26\n",
      "           8       0.93      0.96      0.95        27\n",
      "           9       1.00      0.88      0.94        43\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.97      0.98      0.97       360\n",
      "weighted avg       0.98      0.97      0.97       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        32\n",
      "           1       0.95      0.97      0.96        36\n",
      "           2       0.97      0.97      0.97        38\n",
      "           3       0.98      0.98      0.98        45\n",
      "           4       1.00      1.00      1.00        37\n",
      "           5       0.89      0.97      0.93        33\n",
      "           6       1.00      0.98      0.99        43\n",
      "           7       0.96      1.00      0.98        26\n",
      "           8       0.96      0.93      0.94        27\n",
      "           9       0.97      0.88      0.93        43\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.97      0.97      0.97       360\n",
      "weighted avg       0.97      0.97      0.97       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        31\n",
      "           1       0.90      0.95      0.92        39\n",
      "           2       1.00      0.98      0.99        43\n",
      "           3       0.95      0.97      0.96        40\n",
      "           4       1.00      1.00      1.00        36\n",
      "           5       0.77      0.96      0.86        25\n",
      "           6       1.00      1.00      1.00        34\n",
      "           7       0.95      0.97      0.96        37\n",
      "           8       0.97      0.79      0.87        43\n",
      "           9       0.93      0.88      0.90        32\n",
      "\n",
      "    accuracy                           0.95       360\n",
      "   macro avg       0.94      0.95      0.95       360\n",
      "weighted avg       0.95      0.95      0.95       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        35\n",
      "           1       0.86      0.97      0.91        37\n",
      "           2       0.97      1.00      0.99        39\n",
      "           3       0.95      0.93      0.94        41\n",
      "           4       0.94      0.97      0.95        30\n",
      "           5       0.94      0.94      0.94        33\n",
      "           6       1.00      0.97      0.99        35\n",
      "           7       0.98      0.98      0.98        41\n",
      "           8       0.90      0.79      0.84        33\n",
      "           9       0.89      0.92      0.90        36\n",
      "\n",
      "    accuracy                           0.94       360\n",
      "   macro avg       0.94      0.94      0.94       360\n",
      "weighted avg       0.94      0.94      0.94       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.84      0.90        32\n",
      "           1       0.89      0.94      0.92        36\n",
      "           2       0.92      0.95      0.94        38\n",
      "           3       0.90      0.84      0.87        45\n",
      "           4       0.97      0.95      0.96        37\n",
      "           5       0.83      0.91      0.87        33\n",
      "           6       0.95      0.95      0.95        43\n",
      "           7       0.83      0.96      0.89        26\n",
      "           8       0.73      0.81      0.77        27\n",
      "           9       0.87      0.77      0.81        43\n",
      "\n",
      "    accuracy                           0.89       360\n",
      "   macro avg       0.89      0.89      0.89       360\n",
      "weighted avg       0.89      0.89      0.89       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.97      0.87        30\n",
      "           1       0.86      0.95      0.90        38\n",
      "           2       0.91      0.97      0.94        31\n",
      "           3       0.91      0.87      0.89        46\n",
      "           4       0.97      0.95      0.96        40\n",
      "           5       0.90      0.79      0.84        34\n",
      "           6       0.97      0.90      0.94        42\n",
      "           7       0.95      1.00      0.97        38\n",
      "           8       0.85      0.74      0.79        31\n",
      "           9       0.90      0.87      0.88        30\n",
      "\n",
      "    accuracy                           0.90       360\n",
      "   macro avg       0.90      0.90      0.90       360\n",
      "weighted avg       0.91      0.90      0.90       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.88      0.80        32\n",
      "           1       0.85      0.88      0.86        32\n",
      "           2       0.74      0.88      0.81        33\n",
      "           3       0.71      0.73      0.72        37\n",
      "           4       0.95      0.88      0.91        40\n",
      "           5       0.90      0.83      0.86        42\n",
      "           6       0.88      0.90      0.89        39\n",
      "           7       0.88      0.81      0.84        26\n",
      "           8       0.84      0.53      0.65        40\n",
      "           9       0.68      0.82      0.74        39\n",
      "\n",
      "    accuracy                           0.81       360\n",
      "   macro avg       0.82      0.81      0.81       360\n",
      "weighted avg       0.82      0.81      0.81       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.69      0.67        32\n",
      "           1       0.88      0.78      0.82        36\n",
      "           2       0.83      0.67      0.74        30\n",
      "           3       0.68      0.66      0.67        41\n",
      "           4       0.75      0.84      0.79        32\n",
      "           5       0.88      0.76      0.81        46\n",
      "           6       0.74      0.81      0.78        32\n",
      "           7       0.74      0.80      0.77        40\n",
      "           8       0.62      0.62      0.62        42\n",
      "           9       0.59      0.69      0.63        29\n",
      "\n",
      "    accuracy                           0.73       360\n",
      "   macro avg       0.73      0.73      0.73       360\n",
      "weighted avg       0.74      0.73      0.73       360\n",
      "\n",
      "0.9833333333333333 0 0.98303962701864 3\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "max = 0\n",
    "pre_max = 0\n",
    "loc = 0\n",
    "loc_pre = 0\n",
    "random_forest = RandomForestClassifier(random_state=15)\n",
    "for i in range(15):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(digits_test_arr[i],\n",
    "                                                        digits_label, \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=random.randrange(10,30))\n",
    "    random_forest.fit(X_train, y_train)\n",
    "    y_pred = random_forest.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    pre = precision_score(y_test, y_pred, average = 'macro')\n",
    "    if max < accuracy:\n",
    "        max = accuracy\n",
    "        loc = i\n",
    "    if pre_max < pre:\n",
    "        pre_max = pre\n",
    "        loc_pre = i\n",
    "        \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "print(max, loc, pre_max, loc_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80f3807",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "86b9d1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        37\n",
      "           1       1.00      1.00      1.00        32\n",
      "           2       1.00      1.00      1.00        38\n",
      "           3       1.00      1.00      1.00        43\n",
      "           4       1.00      0.97      0.99        39\n",
      "           5       1.00      0.94      0.97        34\n",
      "           6       1.00      1.00      1.00        29\n",
      "           7       1.00      1.00      1.00        42\n",
      "           8       1.00      1.00      1.00        32\n",
      "           9       0.92      1.00      0.96        34\n",
      "\n",
      "    accuracy                           0.99       360\n",
      "   macro avg       0.99      0.99      0.99       360\n",
      "weighted avg       0.99      0.99      0.99       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        36\n",
      "           1       0.95      1.00      0.97        37\n",
      "           2       1.00      1.00      1.00        31\n",
      "           3       1.00      0.95      0.97        37\n",
      "           4       1.00      0.98      0.99        42\n",
      "           5       0.97      0.97      0.97        34\n",
      "           6       1.00      1.00      1.00        43\n",
      "           7       0.94      1.00      0.97        34\n",
      "           8       0.94      0.94      0.94        35\n",
      "           9       0.97      0.94      0.95        31\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        37\n",
      "           1       0.97      1.00      0.98        31\n",
      "           2       1.00      1.00      1.00        28\n",
      "           3       1.00      1.00      1.00        38\n",
      "           4       1.00      1.00      1.00        41\n",
      "           5       0.97      0.97      0.97        33\n",
      "           6       1.00      0.98      0.99        45\n",
      "           7       1.00      1.00      1.00        40\n",
      "           8       0.97      1.00      0.99        35\n",
      "           9       0.97      0.94      0.95        32\n",
      "\n",
      "    accuracy                           0.99       360\n",
      "   macro avg       0.99      0.99      0.99       360\n",
      "weighted avg       0.99      0.99      0.99       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        32\n",
      "           1       0.97      1.00      0.99        36\n",
      "           2       1.00      1.00      1.00        30\n",
      "           3       1.00      1.00      1.00        41\n",
      "           4       0.97      1.00      0.98        32\n",
      "           5       0.98      1.00      0.99        46\n",
      "           6       1.00      1.00      1.00        32\n",
      "           7       1.00      1.00      1.00        40\n",
      "           8       0.98      0.98      0.98        42\n",
      "           9       1.00      0.93      0.96        29\n",
      "\n",
      "    accuracy                           0.99       360\n",
      "   macro avg       0.99      0.99      0.99       360\n",
      "weighted avg       0.99      0.99      0.99       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        35\n",
      "           1       1.00      1.00      1.00        34\n",
      "           2       1.00      1.00      1.00        35\n",
      "           3       0.97      0.97      0.97        37\n",
      "           4       1.00      1.00      1.00        39\n",
      "           5       0.94      1.00      0.97        30\n",
      "           6       1.00      1.00      1.00        35\n",
      "           7       0.98      0.98      0.98        44\n",
      "           8       1.00      0.94      0.97        36\n",
      "           9       0.97      0.97      0.97        35\n",
      "\n",
      "    accuracy                           0.99       360\n",
      "   macro avg       0.99      0.99      0.99       360\n",
      "weighted avg       0.99      0.99      0.99       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        35\n",
      "           1       0.95      1.00      0.97        37\n",
      "           2       1.00      1.00      1.00        39\n",
      "           3       1.00      0.98      0.99        41\n",
      "           4       0.97      0.93      0.95        30\n",
      "           5       0.94      1.00      0.97        33\n",
      "           6       1.00      0.97      0.99        35\n",
      "           7       0.98      1.00      0.99        41\n",
      "           8       0.91      0.97      0.94        33\n",
      "           9       1.00      0.92      0.96        36\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.97      0.97      0.97       360\n",
      "weighted avg       0.98      0.97      0.98       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        32\n",
      "           1       0.94      1.00      0.97        32\n",
      "           2       1.00      1.00      1.00        33\n",
      "           3       1.00      0.97      0.99        37\n",
      "           4       1.00      0.97      0.99        40\n",
      "           5       0.98      1.00      0.99        42\n",
      "           6       1.00      0.97      0.99        39\n",
      "           7       0.96      1.00      0.98        26\n",
      "           8       0.97      0.95      0.96        40\n",
      "           9       1.00      1.00      1.00        39\n",
      "\n",
      "    accuracy                           0.99       360\n",
      "   macro avg       0.99      0.99      0.99       360\n",
      "weighted avg       0.99      0.99      0.99       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        32\n",
      "           1       0.97      0.95      0.96        40\n",
      "           2       0.94      1.00      0.97        33\n",
      "           3       0.97      0.92      0.95        39\n",
      "           4       1.00      0.98      0.99        44\n",
      "           5       1.00      1.00      1.00        36\n",
      "           6       1.00      0.97      0.98        31\n",
      "           7       0.94      0.97      0.95        30\n",
      "           8       0.95      0.97      0.96        40\n",
      "           9       0.94      0.97      0.96        35\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.97      0.97      0.97       360\n",
      "weighted avg       0.97      0.97      0.97       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        32\n",
      "           1       0.93      0.96      0.95        28\n",
      "           2       0.94      1.00      0.97        29\n",
      "           3       1.00      1.00      1.00        37\n",
      "           4       1.00      1.00      1.00        31\n",
      "           5       0.95      1.00      0.97        35\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       0.98      0.98      0.98        43\n",
      "           8       0.94      0.89      0.92        38\n",
      "           9       0.98      0.92      0.95        50\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.97      0.98      0.97       360\n",
      "weighted avg       0.97      0.97      0.97       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        37\n",
      "           1       0.92      1.00      0.96        34\n",
      "           2       0.97      1.00      0.99        34\n",
      "           3       0.95      0.97      0.96        40\n",
      "           4       0.97      0.94      0.96        34\n",
      "           5       1.00      0.94      0.97        32\n",
      "           6       1.00      0.97      0.99        37\n",
      "           7       1.00      1.00      1.00        40\n",
      "           8       0.91      0.91      0.91        33\n",
      "           9       0.97      0.97      0.97        39\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.97      0.97      0.97       360\n",
      "weighted avg       0.97      0.97      0.97       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97        37\n",
      "           1       0.89      1.00      0.94        32\n",
      "           2       0.97      0.97      0.97        38\n",
      "           3       0.93      0.93      0.93        43\n",
      "           4       1.00      0.92      0.96        39\n",
      "           5       0.97      0.94      0.96        34\n",
      "           6       0.97      1.00      0.98        29\n",
      "           7       0.95      0.98      0.96        42\n",
      "           8       0.88      0.91      0.89        32\n",
      "           9       0.88      0.85      0.87        34\n",
      "\n",
      "    accuracy                           0.94       360\n",
      "   macro avg       0.94      0.94      0.94       360\n",
      "weighted avg       0.95      0.94      0.94       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.93        37\n",
      "           1       0.89      1.00      0.94        32\n",
      "           2       0.95      0.92      0.93        38\n",
      "           3       0.91      0.95      0.93        43\n",
      "           4       0.97      0.87      0.92        39\n",
      "           5       0.89      0.94      0.91        34\n",
      "           6       0.97      0.97      0.97        29\n",
      "           7       0.93      1.00      0.97        42\n",
      "           8       0.90      0.84      0.87        32\n",
      "           9       0.85      0.82      0.84        34\n",
      "\n",
      "    accuracy                           0.92       360\n",
      "   macro avg       0.92      0.92      0.92       360\n",
      "weighted avg       0.92      0.92      0.92       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.93      0.89        30\n",
      "           1       0.90      0.92      0.91        38\n",
      "           2       0.97      0.97      0.97        31\n",
      "           3       0.93      0.87      0.90        46\n",
      "           4       0.95      0.97      0.96        40\n",
      "           5       0.94      0.85      0.89        34\n",
      "           6       0.98      0.95      0.96        42\n",
      "           7       0.90      1.00      0.95        38\n",
      "           8       0.80      0.77      0.79        31\n",
      "           9       0.90      0.87      0.88        30\n",
      "\n",
      "    accuracy                           0.91       360\n",
      "   macro avg       0.91      0.91      0.91       360\n",
      "weighted avg       0.91      0.91      0.91       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.97      0.88        37\n",
      "           1       0.87      0.87      0.87        31\n",
      "           2       0.86      0.89      0.88        28\n",
      "           3       0.84      0.95      0.89        38\n",
      "           4       0.97      0.85      0.91        41\n",
      "           5       0.87      0.79      0.83        33\n",
      "           6       0.90      0.82      0.86        45\n",
      "           7       0.90      0.93      0.91        40\n",
      "           8       0.77      0.66      0.71        35\n",
      "           9       0.76      0.81      0.79        32\n",
      "\n",
      "    accuracy                           0.86       360\n",
      "   macro avg       0.85      0.85      0.85       360\n",
      "weighted avg       0.86      0.86      0.85       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.88      0.62        32\n",
      "           1       0.85      0.82      0.84        28\n",
      "           2       0.76      0.76      0.76        29\n",
      "           3       0.66      0.73      0.69        37\n",
      "           4       0.85      0.94      0.89        31\n",
      "           5       0.90      0.74      0.81        35\n",
      "           6       0.88      0.78      0.83        37\n",
      "           7       0.80      0.74      0.77        43\n",
      "           8       0.64      0.61      0.62        38\n",
      "           9       0.88      0.56      0.68        50\n",
      "\n",
      "    accuracy                           0.74       360\n",
      "   macro avg       0.77      0.76      0.75       360\n",
      "weighted avg       0.77      0.74      0.75       360\n",
      "\n",
      "0.9916666666666667 0 0.991891891891892 0\n"
     ]
    }
   ],
   "source": [
    "max = 0\n",
    "pre_max = 0\n",
    "loc = 0\n",
    "loc_pre = 0\n",
    "for i in range(15):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(digits_test_arr[i],\n",
    "                                                        digits_label, \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=random.randrange(10,30))\n",
    "   \n",
    "    svm_model.fit(X_train, y_train)\n",
    "    y_svm_pred = svm_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_svm_pred)\n",
    "    pre = precision_score(y_test, y_svm_pred, average = 'macro')\n",
    "    if max < accuracy:\n",
    "        max = accuracy\n",
    "        loc = i\n",
    "    if pre_max < pre:\n",
    "        pre_max = pre\n",
    "        loc_pre = i\n",
    "        \n",
    "    print(classification_report(y_test, y_svm_pred))\n",
    "    \n",
    "print(max, loc, pre_max, loc_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f501627",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d6eeaa7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'digits_test_arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-6f3333af802a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mloc_pre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     X_train, X_test, y_train, y_test = train_test_split(digits_test_arr[i],\n\u001b[0m\u001b[0;32m      7\u001b[0m                                                         \u001b[0mdigits_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                                                         \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'digits_test_arr' is not defined"
     ]
    }
   ],
   "source": [
    "max = 0\n",
    "pre_max = 0\n",
    "loc = 0\n",
    "loc_pre = 0\n",
    "for i in range(15):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(digits_test_arr[i],\n",
    "                                                        digits_label, \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=random.randrange(10,30))\n",
    "   \n",
    "    sgd_model.fit(X_train, y_train)\n",
    "    sgd_pred = sgd_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, sgd_pred)\n",
    "    pre = precision_score(y_test, sgd_pred, average = 'macro')\n",
    "    if max < accuracy:\n",
    "        max = accuracy\n",
    "        loc = i\n",
    "    if pre_max < pre:\n",
    "        pre_max = pre\n",
    "        loc_pre = i\n",
    "        \n",
    "    print(classification_report(y_test, sgd_pred))\n",
    "    \n",
    "print(max, loc, pre_max, loc_pre)\n",
    "digits_test_arr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b5e8a2",
   "metadata": {},
   "source": [
    "# 총 네가지 모델을 돌려본 결과 가장 좋은 결과를 보인 모델은 SVM이다\n",
    "\n",
    "# 모델은 SVM이고, 데이터 조건은 글씨 범위를 2~16으로 했을때,\n",
    "\n",
    "# accuracy 0.9916666666666667 precision  0.991891891891892 나왔다.\n",
    "\n",
    "# 이때, 내가 precision을 recall보다 중요하게 본 이유는\n",
    "\n",
    "# 손글씨를 판단할때 예측을 정확하게 하는 편이 중요하다고 생각했기 때문이다.\n",
    "\n",
    "# 즉, 내가 내린 결론은 손글씨 판단하는 모델은 SVM이 적합하고,\n",
    "\n",
    "# 데이터는 글씨 범위를 2~16이 적합하다고 결론을 내렸습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c061a3",
   "metadata": {},
   "source": [
    "# 이제 와인값을 시작하겠습니다.\n",
    "\n",
    "# 먼저 와인값을 넣어주고 데이터, 키값을 확인하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6da8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fea4470",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74c83929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names'])\n"
     ]
    }
   ],
   "source": [
    "print(wine_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ad7804c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class_0' 'class_1' 'class_2']\n"
     ]
    }
   ],
   "source": [
    "print(wine_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea89e490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n"
     ]
    }
   ],
   "source": [
    "print(wine_data.feature_names)\n",
    "wine_feature = wine_data.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d7991",
   "metadata": {},
   "source": [
    "# 일단 와인 데이터가 어떻게 되어있는지 궁금해서 판다스를 이용해\n",
    "\n",
    "# 데이터를 보기로 했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e094fc3",
   "metadata": {},
   "source": [
    "# alcohol은 도수\n",
    "\n",
    "# malic_acid 은 사과산 - 익기전에는 높고 익으면 익을수록 수치가 낮아진다.\n",
    "\n",
    "### ash는 550~600도로 전기분해 후 유기물을 제외한 나머지 무기물 -  포도가 재배되는 땅의 품질\n",
    "\n",
    "# alcalinity_of_ash 앞에 있는 무기물중 알칼리성 무기물\n",
    "\n",
    "# magnesium 마그네슘 \n",
    "\n",
    "# total_phenols 와인 맛, 향, 바디감(얼마나 끈적이는가)에 영향을 준다\n",
    " \n",
    "# flavanoids 항산화을 뜻한다\n",
    "\n",
    "# nonflavanoid_phenols 항산화가 없는 페놀...\n",
    "\n",
    "# proanthocyanins total_phenols의 포함된 성분중 상세표기\n",
    "\n",
    "# color_intensity 와인의 색 세기\n",
    "\n",
    "# hue 색상....\n",
    "\n",
    "# od280/od315_of_diluted_wines\t희석 와인의 OD280/OD315 비율(정말 모르겠다)\n",
    "\n",
    "# proline 와인 성분중 하나인데...(뭔 성분인지는 정확히는 모르겠다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec01cb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
       "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
       "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
       "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
       "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  label  \n",
       "0                            3.92   1065.0      0  \n",
       "1                            3.40   1050.0      0  \n",
       "2                            3.17   1185.0      0  \n",
       "3                            3.45   1480.0      0  \n",
       "4                            2.93    735.0      0  \n",
       "..                            ...      ...    ...  \n",
       "173                          1.74    740.0      2  \n",
       "174                          1.56    750.0      2  \n",
       "175                          1.56    835.0      2  \n",
       "176                          1.62    840.0      2  \n",
       "177                          1.60    560.0      2  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "wine_df = pd.DataFrame(data = wine_data.data, columns = wine_feature)\n",
    "wine_df[\"label\"] = wine_data.target\n",
    "wine_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1b8e59",
   "metadata": {},
   "source": [
    "# 자료를 보니 와인의 여러 성분, 특징들로 와인을 3가지로 구분했다.\n",
    "\n",
    "# 그래서 먼저, 데이터를 target_names기준으로 3가지로 나누겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7297731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(wine_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07b1dca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-15e497843984>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwine_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m59\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwine_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwine_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "print(wine_data.target[59])\n",
    "print(np.where(wine_data.target == 1))\n",
    "print(np.where(wine_data.target == 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552be27f",
   "metadata": {},
   "source": [
    "## 와인 0번: 0~58\n",
    "\n",
    "## 와인 1번: 59~129\n",
    "\n",
    "## 와인 2번: 130 ~177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f2c99fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.23\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.20</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.45</td>\n",
       "      <td>15.2</td>\n",
       "      <td>112.0</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.97</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1450.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.39</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.45</td>\n",
       "      <td>14.6</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1.02</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1290.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.06</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.61</td>\n",
       "      <td>17.6</td>\n",
       "      <td>121.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.05</td>\n",
       "      <td>1.06</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1295.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.83</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.17</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13.86</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.27</td>\n",
       "      <td>16.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.85</td>\n",
       "      <td>7.22</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14.10</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.30</td>\n",
       "      <td>18.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5.75</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14.12</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.32</td>\n",
       "      <td>16.8</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.57</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.75</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.41</td>\n",
       "      <td>16.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.81</td>\n",
       "      <td>5.60</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.75</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.39</td>\n",
       "      <td>11.4</td>\n",
       "      <td>91.0</td>\n",
       "      <td>3.10</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.40</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.73</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.38</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.38</td>\n",
       "      <td>12.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.96</td>\n",
       "      <td>7.50</td>\n",
       "      <td>1.20</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1547.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13.63</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.70</td>\n",
       "      <td>17.2</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.46</td>\n",
       "      <td>7.30</td>\n",
       "      <td>1.28</td>\n",
       "      <td>2.88</td>\n",
       "      <td>1310.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.30</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2.72</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.97</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.07</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13.83</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.62</td>\n",
       "      <td>20.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.72</td>\n",
       "      <td>6.60</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2.57</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.19</td>\n",
       "      <td>1.59</td>\n",
       "      <td>2.48</td>\n",
       "      <td>16.5</td>\n",
       "      <td>108.0</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.86</td>\n",
       "      <td>8.70</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.64</td>\n",
       "      <td>3.10</td>\n",
       "      <td>2.56</td>\n",
       "      <td>15.2</td>\n",
       "      <td>116.0</td>\n",
       "      <td>2.70</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.66</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.96</td>\n",
       "      <td>3.36</td>\n",
       "      <td>845.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.06</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.28</td>\n",
       "      <td>16.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.10</td>\n",
       "      <td>5.65</td>\n",
       "      <td>1.09</td>\n",
       "      <td>3.71</td>\n",
       "      <td>780.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12.93</td>\n",
       "      <td>3.80</td>\n",
       "      <td>2.65</td>\n",
       "      <td>18.6</td>\n",
       "      <td>102.0</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.98</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.52</td>\n",
       "      <td>770.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>13.71</td>\n",
       "      <td>1.86</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.88</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.69</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1.11</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12.85</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.52</td>\n",
       "      <td>17.8</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2.48</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.46</td>\n",
       "      <td>3.93</td>\n",
       "      <td>1.09</td>\n",
       "      <td>3.63</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>13.50</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.61</td>\n",
       "      <td>20.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.53</td>\n",
       "      <td>2.61</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.52</td>\n",
       "      <td>1.12</td>\n",
       "      <td>3.82</td>\n",
       "      <td>845.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13.05</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.22</td>\n",
       "      <td>25.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1.13</td>\n",
       "      <td>3.20</td>\n",
       "      <td>830.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13.39</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.62</td>\n",
       "      <td>16.1</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.45</td>\n",
       "      <td>4.80</td>\n",
       "      <td>0.92</td>\n",
       "      <td>3.22</td>\n",
       "      <td>1195.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13.30</td>\n",
       "      <td>1.72</td>\n",
       "      <td>2.14</td>\n",
       "      <td>17.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.35</td>\n",
       "      <td>3.95</td>\n",
       "      <td>1.02</td>\n",
       "      <td>2.77</td>\n",
       "      <td>1285.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13.87</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.80</td>\n",
       "      <td>19.4</td>\n",
       "      <td>107.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.97</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.76</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.40</td>\n",
       "      <td>915.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>14.02</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.21</td>\n",
       "      <td>16.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.98</td>\n",
       "      <td>4.70</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>13.73</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.70</td>\n",
       "      <td>22.5</td>\n",
       "      <td>101.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5.70</td>\n",
       "      <td>1.19</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1285.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>13.58</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.36</td>\n",
       "      <td>19.1</td>\n",
       "      <td>106.0</td>\n",
       "      <td>2.86</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.95</td>\n",
       "      <td>6.90</td>\n",
       "      <td>1.09</td>\n",
       "      <td>2.88</td>\n",
       "      <td>1515.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>13.68</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.36</td>\n",
       "      <td>17.2</td>\n",
       "      <td>104.0</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.97</td>\n",
       "      <td>3.84</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.87</td>\n",
       "      <td>990.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>13.76</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.70</td>\n",
       "      <td>19.5</td>\n",
       "      <td>132.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.74</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.35</td>\n",
       "      <td>5.40</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>13.51</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.65</td>\n",
       "      <td>19.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.53</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.54</td>\n",
       "      <td>4.20</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2.87</td>\n",
       "      <td>1095.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>13.48</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.41</td>\n",
       "      <td>20.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.86</td>\n",
       "      <td>5.10</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.47</td>\n",
       "      <td>920.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>13.28</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.84</td>\n",
       "      <td>15.5</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.36</td>\n",
       "      <td>4.60</td>\n",
       "      <td>1.09</td>\n",
       "      <td>2.78</td>\n",
       "      <td>880.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>13.05</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.55</td>\n",
       "      <td>18.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.44</td>\n",
       "      <td>4.25</td>\n",
       "      <td>1.12</td>\n",
       "      <td>2.51</td>\n",
       "      <td>1105.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>13.07</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.10</td>\n",
       "      <td>15.5</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.37</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.18</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>14.22</td>\n",
       "      <td>3.99</td>\n",
       "      <td>2.51</td>\n",
       "      <td>13.2</td>\n",
       "      <td>128.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.08</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.89</td>\n",
       "      <td>3.53</td>\n",
       "      <td>760.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>13.56</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.31</td>\n",
       "      <td>16.2</td>\n",
       "      <td>117.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.34</td>\n",
       "      <td>2.34</td>\n",
       "      <td>6.13</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3.38</td>\n",
       "      <td>795.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>13.41</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.12</td>\n",
       "      <td>18.8</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.48</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.91</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>13.88</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.59</td>\n",
       "      <td>15.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.70</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.88</td>\n",
       "      <td>3.56</td>\n",
       "      <td>1095.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>13.24</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.29</td>\n",
       "      <td>17.5</td>\n",
       "      <td>103.0</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4.36</td>\n",
       "      <td>0.82</td>\n",
       "      <td>3.00</td>\n",
       "      <td>680.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>13.05</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.10</td>\n",
       "      <td>17.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.03</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.88</td>\n",
       "      <td>3.35</td>\n",
       "      <td>885.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>14.21</td>\n",
       "      <td>4.04</td>\n",
       "      <td>2.44</td>\n",
       "      <td>18.9</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.24</td>\n",
       "      <td>0.87</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>14.38</td>\n",
       "      <td>3.59</td>\n",
       "      <td>2.28</td>\n",
       "      <td>16.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.27</td>\n",
       "      <td>2.19</td>\n",
       "      <td>4.90</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.44</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>13.90</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.12</td>\n",
       "      <td>16.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>3.10</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.14</td>\n",
       "      <td>6.10</td>\n",
       "      <td>0.91</td>\n",
       "      <td>3.33</td>\n",
       "      <td>985.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>14.10</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.40</td>\n",
       "      <td>18.8</td>\n",
       "      <td>103.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2.38</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.07</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1060.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>13.94</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.27</td>\n",
       "      <td>17.4</td>\n",
       "      <td>108.0</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2.08</td>\n",
       "      <td>8.90</td>\n",
       "      <td>1.12</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>13.05</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.04</td>\n",
       "      <td>12.4</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2.72</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.17</td>\n",
       "      <td>2.91</td>\n",
       "      <td>7.20</td>\n",
       "      <td>1.12</td>\n",
       "      <td>2.91</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>13.83</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.60</td>\n",
       "      <td>17.2</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.60</td>\n",
       "      <td>1.24</td>\n",
       "      <td>3.37</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>13.82</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.42</td>\n",
       "      <td>14.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>3.88</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.87</td>\n",
       "      <td>7.05</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>13.77</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.68</td>\n",
       "      <td>17.1</td>\n",
       "      <td>115.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.68</td>\n",
       "      <td>6.30</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2.93</td>\n",
       "      <td>1375.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>13.74</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.25</td>\n",
       "      <td>16.4</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.62</td>\n",
       "      <td>5.85</td>\n",
       "      <td>0.92</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1060.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>13.56</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.46</td>\n",
       "      <td>20.5</td>\n",
       "      <td>116.0</td>\n",
       "      <td>2.96</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.45</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3.03</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>14.22</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2.30</td>\n",
       "      <td>16.3</td>\n",
       "      <td>118.0</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>2.03</td>\n",
       "      <td>6.38</td>\n",
       "      <td>0.94</td>\n",
       "      <td>3.31</td>\n",
       "      <td>970.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>13.29</td>\n",
       "      <td>1.97</td>\n",
       "      <td>2.68</td>\n",
       "      <td>16.8</td>\n",
       "      <td>102.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.66</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.07</td>\n",
       "      <td>2.84</td>\n",
       "      <td>1270.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>13.72</td>\n",
       "      <td>1.43</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.7</td>\n",
       "      <td>108.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2.04</td>\n",
       "      <td>6.80</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.87</td>\n",
       "      <td>1285.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0     14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1     13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2     13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3     14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4     13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "5     14.20        1.76  2.45               15.2      112.0           3.27   \n",
       "6     14.39        1.87  2.45               14.6       96.0           2.50   \n",
       "7     14.06        2.15  2.61               17.6      121.0           2.60   \n",
       "8     14.83        1.64  2.17               14.0       97.0           2.80   \n",
       "9     13.86        1.35  2.27               16.0       98.0           2.98   \n",
       "10    14.10        2.16  2.30               18.0      105.0           2.95   \n",
       "11    14.12        1.48  2.32               16.8       95.0           2.20   \n",
       "12    13.75        1.73  2.41               16.0       89.0           2.60   \n",
       "13    14.75        1.73  2.39               11.4       91.0           3.10   \n",
       "14    14.38        1.87  2.38               12.0      102.0           3.30   \n",
       "15    13.63        1.81  2.70               17.2      112.0           2.85   \n",
       "16    14.30        1.92  2.72               20.0      120.0           2.80   \n",
       "17    13.83        1.57  2.62               20.0      115.0           2.95   \n",
       "18    14.19        1.59  2.48               16.5      108.0           3.30   \n",
       "19    13.64        3.10  2.56               15.2      116.0           2.70   \n",
       "20    14.06        1.63  2.28               16.0      126.0           3.00   \n",
       "21    12.93        3.80  2.65               18.6      102.0           2.41   \n",
       "22    13.71        1.86  2.36               16.6      101.0           2.61   \n",
       "23    12.85        1.60  2.52               17.8       95.0           2.48   \n",
       "24    13.50        1.81  2.61               20.0       96.0           2.53   \n",
       "25    13.05        2.05  3.22               25.0      124.0           2.63   \n",
       "26    13.39        1.77  2.62               16.1       93.0           2.85   \n",
       "27    13.30        1.72  2.14               17.0       94.0           2.40   \n",
       "28    13.87        1.90  2.80               19.4      107.0           2.95   \n",
       "29    14.02        1.68  2.21               16.0       96.0           2.65   \n",
       "30    13.73        1.50  2.70               22.5      101.0           3.00   \n",
       "31    13.58        1.66  2.36               19.1      106.0           2.86   \n",
       "32    13.68        1.83  2.36               17.2      104.0           2.42   \n",
       "33    13.76        1.53  2.70               19.5      132.0           2.95   \n",
       "34    13.51        1.80  2.65               19.0      110.0           2.35   \n",
       "35    13.48        1.81  2.41               20.5      100.0           2.70   \n",
       "36    13.28        1.64  2.84               15.5      110.0           2.60   \n",
       "37    13.05        1.65  2.55               18.0       98.0           2.45   \n",
       "38    13.07        1.50  2.10               15.5       98.0           2.40   \n",
       "39    14.22        3.99  2.51               13.2      128.0           3.00   \n",
       "40    13.56        1.71  2.31               16.2      117.0           3.15   \n",
       "41    13.41        3.84  2.12               18.8       90.0           2.45   \n",
       "42    13.88        1.89  2.59               15.0      101.0           3.25   \n",
       "43    13.24        3.98  2.29               17.5      103.0           2.64   \n",
       "44    13.05        1.77  2.10               17.0      107.0           3.00   \n",
       "45    14.21        4.04  2.44               18.9      111.0           2.85   \n",
       "46    14.38        3.59  2.28               16.0      102.0           3.25   \n",
       "47    13.90        1.68  2.12               16.0      101.0           3.10   \n",
       "48    14.10        2.02  2.40               18.8      103.0           2.75   \n",
       "49    13.94        1.73  2.27               17.4      108.0           2.88   \n",
       "50    13.05        1.73  2.04               12.4       92.0           2.72   \n",
       "51    13.83        1.65  2.60               17.2       94.0           2.45   \n",
       "52    13.82        1.75  2.42               14.0      111.0           3.88   \n",
       "53    13.77        1.90  2.68               17.1      115.0           3.00   \n",
       "54    13.74        1.67  2.25               16.4      118.0           2.60   \n",
       "55    13.56        1.73  2.46               20.5      116.0           2.96   \n",
       "56    14.22        1.70  2.30               16.3      118.0           3.20   \n",
       "57    13.29        1.97  2.68               16.8      102.0           3.00   \n",
       "58    13.72        1.43  2.50               16.7      108.0           3.40   \n",
       "\n",
       "    flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0         3.06                  0.28             2.29             5.64  1.04   \n",
       "1         2.76                  0.26             1.28             4.38  1.05   \n",
       "2         3.24                  0.30             2.81             5.68  1.03   \n",
       "3         3.49                  0.24             2.18             7.80  0.86   \n",
       "4         2.69                  0.39             1.82             4.32  1.04   \n",
       "5         3.39                  0.34             1.97             6.75  1.05   \n",
       "6         2.52                  0.30             1.98             5.25  1.02   \n",
       "7         2.51                  0.31             1.25             5.05  1.06   \n",
       "8         2.98                  0.29             1.98             5.20  1.08   \n",
       "9         3.15                  0.22             1.85             7.22  1.01   \n",
       "10        3.32                  0.22             2.38             5.75  1.25   \n",
       "11        2.43                  0.26             1.57             5.00  1.17   \n",
       "12        2.76                  0.29             1.81             5.60  1.15   \n",
       "13        3.69                  0.43             2.81             5.40  1.25   \n",
       "14        3.64                  0.29             2.96             7.50  1.20   \n",
       "15        2.91                  0.30             1.46             7.30  1.28   \n",
       "16        3.14                  0.33             1.97             6.20  1.07   \n",
       "17        3.40                  0.40             1.72             6.60  1.13   \n",
       "18        3.93                  0.32             1.86             8.70  1.23   \n",
       "19        3.03                  0.17             1.66             5.10  0.96   \n",
       "20        3.17                  0.24             2.10             5.65  1.09   \n",
       "21        2.41                  0.25             1.98             4.50  1.03   \n",
       "22        2.88                  0.27             1.69             3.80  1.11   \n",
       "23        2.37                  0.26             1.46             3.93  1.09   \n",
       "24        2.61                  0.28             1.66             3.52  1.12   \n",
       "25        2.68                  0.47             1.92             3.58  1.13   \n",
       "26        2.94                  0.34             1.45             4.80  0.92   \n",
       "27        2.19                  0.27             1.35             3.95  1.02   \n",
       "28        2.97                  0.37             1.76             4.50  1.25   \n",
       "29        2.33                  0.26             1.98             4.70  1.04   \n",
       "30        3.25                  0.29             2.38             5.70  1.19   \n",
       "31        3.19                  0.22             1.95             6.90  1.09   \n",
       "32        2.69                  0.42             1.97             3.84  1.23   \n",
       "33        2.74                  0.50             1.35             5.40  1.25   \n",
       "34        2.53                  0.29             1.54             4.20  1.10   \n",
       "35        2.98                  0.26             1.86             5.10  1.04   \n",
       "36        2.68                  0.34             1.36             4.60  1.09   \n",
       "37        2.43                  0.29             1.44             4.25  1.12   \n",
       "38        2.64                  0.28             1.37             3.70  1.18   \n",
       "39        3.04                  0.20             2.08             5.10  0.89   \n",
       "40        3.29                  0.34             2.34             6.13  0.95   \n",
       "41        2.68                  0.27             1.48             4.28  0.91   \n",
       "42        3.56                  0.17             1.70             5.43  0.88   \n",
       "43        2.63                  0.32             1.66             4.36  0.82   \n",
       "44        3.00                  0.28             2.03             5.04  0.88   \n",
       "45        2.65                  0.30             1.25             5.24  0.87   \n",
       "46        3.17                  0.27             2.19             4.90  1.04   \n",
       "47        3.39                  0.21             2.14             6.10  0.91   \n",
       "48        2.92                  0.32             2.38             6.20  1.07   \n",
       "49        3.54                  0.32             2.08             8.90  1.12   \n",
       "50        3.27                  0.17             2.91             7.20  1.12   \n",
       "51        2.99                  0.22             2.29             5.60  1.24   \n",
       "52        3.74                  0.32             1.87             7.05  1.01   \n",
       "53        2.79                  0.39             1.68             6.30  1.13   \n",
       "54        2.90                  0.21             1.62             5.85  0.92   \n",
       "55        2.78                  0.20             2.45             6.25  0.98   \n",
       "56        3.00                  0.26             2.03             6.38  0.94   \n",
       "57        3.23                  0.31             1.66             6.00  1.07   \n",
       "58        3.67                  0.19             2.04             6.80  0.89   \n",
       "\n",
       "    od280/od315_of_diluted_wines  proline  label  \n",
       "0                           3.92   1065.0      0  \n",
       "1                           3.40   1050.0      0  \n",
       "2                           3.17   1185.0      0  \n",
       "3                           3.45   1480.0      0  \n",
       "4                           2.93    735.0      0  \n",
       "5                           2.85   1450.0      0  \n",
       "6                           3.58   1290.0      0  \n",
       "7                           3.58   1295.0      0  \n",
       "8                           2.85   1045.0      0  \n",
       "9                           3.55   1045.0      0  \n",
       "10                          3.17   1510.0      0  \n",
       "11                          2.82   1280.0      0  \n",
       "12                          2.90   1320.0      0  \n",
       "13                          2.73   1150.0      0  \n",
       "14                          3.00   1547.0      0  \n",
       "15                          2.88   1310.0      0  \n",
       "16                          2.65   1280.0      0  \n",
       "17                          2.57   1130.0      0  \n",
       "18                          2.82   1680.0      0  \n",
       "19                          3.36    845.0      0  \n",
       "20                          3.71    780.0      0  \n",
       "21                          3.52    770.0      0  \n",
       "22                          4.00   1035.0      0  \n",
       "23                          3.63   1015.0      0  \n",
       "24                          3.82    845.0      0  \n",
       "25                          3.20    830.0      0  \n",
       "26                          3.22   1195.0      0  \n",
       "27                          2.77   1285.0      0  \n",
       "28                          3.40    915.0      0  \n",
       "29                          3.59   1035.0      0  \n",
       "30                          2.71   1285.0      0  \n",
       "31                          2.88   1515.0      0  \n",
       "32                          2.87    990.0      0  \n",
       "33                          3.00   1235.0      0  \n",
       "34                          2.87   1095.0      0  \n",
       "35                          3.47    920.0      0  \n",
       "36                          2.78    880.0      0  \n",
       "37                          2.51   1105.0      0  \n",
       "38                          2.69   1020.0      0  \n",
       "39                          3.53    760.0      0  \n",
       "40                          3.38    795.0      0  \n",
       "41                          3.00   1035.0      0  \n",
       "42                          3.56   1095.0      0  \n",
       "43                          3.00    680.0      0  \n",
       "44                          3.35    885.0      0  \n",
       "45                          3.33   1080.0      0  \n",
       "46                          3.44   1065.0      0  \n",
       "47                          3.33    985.0      0  \n",
       "48                          2.75   1060.0      0  \n",
       "49                          3.10   1260.0      0  \n",
       "50                          2.91   1150.0      0  \n",
       "51                          3.37   1265.0      0  \n",
       "52                          3.26   1190.0      0  \n",
       "53                          2.93   1375.0      0  \n",
       "54                          3.20   1060.0      0  \n",
       "55                          3.03   1120.0      0  \n",
       "56                          3.31    970.0      0  \n",
       "57                          2.84   1270.0      0  \n",
       "58                          2.87   1285.0      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_0 = wine_data.data[:59]\n",
    "wine_t_0 = wine_data.target[:59]\n",
    "wine_df_0 = pd.DataFrame(data = wine_0, columns = wine_feature)\n",
    "wine_df_0[\"label\"] = wine_t_0\n",
    "print(wine_df_0.iloc[0]['alcohol'])\n",
    "wine_df_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "748284eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.37</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.36</td>\n",
       "      <td>10.6</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.82</td>\n",
       "      <td>520.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.33</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2.28</td>\n",
       "      <td>16.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.41</td>\n",
       "      <td>3.27</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.67</td>\n",
       "      <td>680.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.64</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2.02</td>\n",
       "      <td>16.8</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.02</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.62</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.59</td>\n",
       "      <td>450.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.67</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.92</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.73</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.46</td>\n",
       "      <td>630.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.37</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2.16</td>\n",
       "      <td>19.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.87</td>\n",
       "      <td>4.45</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.87</td>\n",
       "      <td>420.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>12.07</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.17</td>\n",
       "      <td>21.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.28</td>\n",
       "      <td>378.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>12.43</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.29</td>\n",
       "      <td>21.5</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2.74</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.77</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.69</td>\n",
       "      <td>2.84</td>\n",
       "      <td>352.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>11.79</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.78</td>\n",
       "      <td>28.5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.76</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2.44</td>\n",
       "      <td>466.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>12.37</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.30</td>\n",
       "      <td>24.5</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.78</td>\n",
       "      <td>342.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>12.04</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.38</td>\n",
       "      <td>22.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.79</td>\n",
       "      <td>2.57</td>\n",
       "      <td>580.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0     12.37        0.94  1.36               10.6       88.0           1.98   \n",
       "1     12.33        1.10  2.28               16.0      101.0           2.05   \n",
       "2     12.64        1.36  2.02               16.8      100.0           2.02   \n",
       "3     13.67        1.25  1.92               18.0       94.0           2.10   \n",
       "4     12.37        1.13  2.16               19.0       87.0           3.50   \n",
       "..      ...         ...   ...                ...        ...            ...   \n",
       "66    12.07        2.16  2.17               21.0       85.0           2.60   \n",
       "67    12.43        1.53  2.29               21.5       86.0           2.74   \n",
       "68    11.79        2.13  2.78               28.5       92.0           2.13   \n",
       "69    12.37        1.63  2.30               24.5       88.0           2.22   \n",
       "70    12.04        4.30  2.38               22.0       80.0           2.10   \n",
       "\n",
       "    flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0         0.57                  0.28             0.42             1.95  1.05   \n",
       "1         1.09                  0.63             0.41             3.27  1.25   \n",
       "2         1.41                  0.53             0.62             5.75  0.98   \n",
       "3         1.79                  0.32             0.73             3.80  1.23   \n",
       "4         3.10                  0.19             1.87             4.45  1.22   \n",
       "..         ...                   ...              ...              ...   ...   \n",
       "66        2.65                  0.37             1.35             2.76  0.86   \n",
       "67        3.15                  0.39             1.77             3.94  0.69   \n",
       "68        2.24                  0.58             1.76             3.00  0.97   \n",
       "69        2.45                  0.40             1.90             2.12  0.89   \n",
       "70        1.75                  0.42             1.35             2.60  0.79   \n",
       "\n",
       "    od280/od315_of_diluted_wines  proline  label  \n",
       "0                           1.82    520.0      1  \n",
       "1                           1.67    680.0      1  \n",
       "2                           1.59    450.0      1  \n",
       "3                           2.46    630.0      1  \n",
       "4                           2.87    420.0      1  \n",
       "..                           ...      ...    ...  \n",
       "66                          3.28    378.0      1  \n",
       "67                          2.84    352.0      1  \n",
       "68                          2.44    466.0      1  \n",
       "69                          2.78    342.0      1  \n",
       "70                          2.57    580.0      1  \n",
       "\n",
       "[71 rows x 14 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_1 = wine_data.data[59:130]\n",
    "wine_t_1 = wine_data.target[59:130]\n",
    "wine_df_1 = pd.DataFrame(data = wine_1, columns = wine_feature)\n",
    "wine_df_1[\"label\"] = wine_t_1\n",
    "wine_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b8c6ca6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.86</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.32</td>\n",
       "      <td>18.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.94</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.29</td>\n",
       "      <td>630.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.88</td>\n",
       "      <td>2.99</td>\n",
       "      <td>2.40</td>\n",
       "      <td>20.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.83</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.42</td>\n",
       "      <td>530.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.81</td>\n",
       "      <td>2.31</td>\n",
       "      <td>2.40</td>\n",
       "      <td>24.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.83</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.36</td>\n",
       "      <td>560.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.70</td>\n",
       "      <td>3.55</td>\n",
       "      <td>2.36</td>\n",
       "      <td>21.5</td>\n",
       "      <td>106.0</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.84</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.29</td>\n",
       "      <td>600.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.51</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2.25</td>\n",
       "      <td>17.5</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.450000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.51</td>\n",
       "      <td>650.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.60</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.20</td>\n",
       "      <td>18.5</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.94</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1.58</td>\n",
       "      <td>695.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12.25</td>\n",
       "      <td>4.72</td>\n",
       "      <td>2.54</td>\n",
       "      <td>21.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.80</td>\n",
       "      <td>3.850000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.27</td>\n",
       "      <td>720.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.53</td>\n",
       "      <td>5.51</td>\n",
       "      <td>2.64</td>\n",
       "      <td>25.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.10</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1.69</td>\n",
       "      <td>515.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.49</td>\n",
       "      <td>3.59</td>\n",
       "      <td>2.19</td>\n",
       "      <td>19.5</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.82</td>\n",
       "      <td>580.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.84</td>\n",
       "      <td>2.96</td>\n",
       "      <td>2.61</td>\n",
       "      <td>24.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.81</td>\n",
       "      <td>4.920000</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.15</td>\n",
       "      <td>590.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12.93</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2.70</td>\n",
       "      <td>21.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.75</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.77</td>\n",
       "      <td>2.31</td>\n",
       "      <td>600.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13.36</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.35</td>\n",
       "      <td>20.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.64</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2.47</td>\n",
       "      <td>780.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.52</td>\n",
       "      <td>3.17</td>\n",
       "      <td>2.72</td>\n",
       "      <td>23.5</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.55</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.06</td>\n",
       "      <td>520.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.62</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.35</td>\n",
       "      <td>20.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.02</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>0.91</td>\n",
       "      <td>2.05</td>\n",
       "      <td>550.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12.25</td>\n",
       "      <td>3.88</td>\n",
       "      <td>2.20</td>\n",
       "      <td>18.5</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.14</td>\n",
       "      <td>8.210000</td>\n",
       "      <td>0.65</td>\n",
       "      <td>2.00</td>\n",
       "      <td>855.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13.16</td>\n",
       "      <td>3.57</td>\n",
       "      <td>2.15</td>\n",
       "      <td>21.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.30</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.68</td>\n",
       "      <td>830.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13.88</td>\n",
       "      <td>5.04</td>\n",
       "      <td>2.23</td>\n",
       "      <td>20.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.68</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.33</td>\n",
       "      <td>415.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>12.87</td>\n",
       "      <td>4.61</td>\n",
       "      <td>2.48</td>\n",
       "      <td>21.5</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.86</td>\n",
       "      <td>7.650000</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.86</td>\n",
       "      <td>625.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13.32</td>\n",
       "      <td>3.24</td>\n",
       "      <td>2.38</td>\n",
       "      <td>21.5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.25</td>\n",
       "      <td>8.420000</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.62</td>\n",
       "      <td>650.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.08</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.36</td>\n",
       "      <td>21.5</td>\n",
       "      <td>113.0</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.14</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.33</td>\n",
       "      <td>550.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.50</td>\n",
       "      <td>3.12</td>\n",
       "      <td>2.62</td>\n",
       "      <td>24.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.25</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.30</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12.79</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2.48</td>\n",
       "      <td>22.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.26</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.47</td>\n",
       "      <td>480.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>13.11</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.75</td>\n",
       "      <td>25.5</td>\n",
       "      <td>116.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.56</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.33</td>\n",
       "      <td>425.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>13.23</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.28</td>\n",
       "      <td>18.5</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.87</td>\n",
       "      <td>10.520000</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.51</td>\n",
       "      <td>675.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>12.58</td>\n",
       "      <td>1.29</td>\n",
       "      <td>2.10</td>\n",
       "      <td>20.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.40</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.55</td>\n",
       "      <td>640.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13.17</td>\n",
       "      <td>5.19</td>\n",
       "      <td>2.32</td>\n",
       "      <td>22.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.55</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.48</td>\n",
       "      <td>725.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13.84</td>\n",
       "      <td>4.12</td>\n",
       "      <td>2.38</td>\n",
       "      <td>19.5</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.56</td>\n",
       "      <td>9.010000</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.64</td>\n",
       "      <td>480.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>12.45</td>\n",
       "      <td>3.03</td>\n",
       "      <td>2.64</td>\n",
       "      <td>27.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.14</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.73</td>\n",
       "      <td>880.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>14.34</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.70</td>\n",
       "      <td>25.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2.70</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.96</td>\n",
       "      <td>660.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>13.48</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.64</td>\n",
       "      <td>22.5</td>\n",
       "      <td>89.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2.29</td>\n",
       "      <td>11.750000</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.78</td>\n",
       "      <td>620.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>12.36</td>\n",
       "      <td>3.83</td>\n",
       "      <td>2.38</td>\n",
       "      <td>21.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.04</td>\n",
       "      <td>7.650000</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.58</td>\n",
       "      <td>520.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>13.69</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.54</td>\n",
       "      <td>20.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.80</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.82</td>\n",
       "      <td>680.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>12.85</td>\n",
       "      <td>3.27</td>\n",
       "      <td>2.58</td>\n",
       "      <td>22.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.96</td>\n",
       "      <td>5.580000</td>\n",
       "      <td>0.87</td>\n",
       "      <td>2.11</td>\n",
       "      <td>570.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>12.96</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2.35</td>\n",
       "      <td>18.5</td>\n",
       "      <td>106.0</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.94</td>\n",
       "      <td>5.280000</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.75</td>\n",
       "      <td>675.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>13.78</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.30</td>\n",
       "      <td>22.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.03</td>\n",
       "      <td>9.580000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.68</td>\n",
       "      <td>615.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>13.73</td>\n",
       "      <td>4.36</td>\n",
       "      <td>2.26</td>\n",
       "      <td>22.5</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.15</td>\n",
       "      <td>6.620000</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.75</td>\n",
       "      <td>520.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>13.45</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.60</td>\n",
       "      <td>23.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.46</td>\n",
       "      <td>10.680000</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.56</td>\n",
       "      <td>695.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>12.82</td>\n",
       "      <td>3.37</td>\n",
       "      <td>2.30</td>\n",
       "      <td>19.5</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.97</td>\n",
       "      <td>10.260000</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.75</td>\n",
       "      <td>685.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>13.58</td>\n",
       "      <td>2.58</td>\n",
       "      <td>2.69</td>\n",
       "      <td>24.5</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.54</td>\n",
       "      <td>8.660000</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.80</td>\n",
       "      <td>750.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>13.40</td>\n",
       "      <td>4.60</td>\n",
       "      <td>2.86</td>\n",
       "      <td>25.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.11</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.92</td>\n",
       "      <td>630.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>12.20</td>\n",
       "      <td>3.03</td>\n",
       "      <td>2.32</td>\n",
       "      <td>19.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.73</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.83</td>\n",
       "      <td>510.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>12.77</td>\n",
       "      <td>2.39</td>\n",
       "      <td>2.28</td>\n",
       "      <td>19.5</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.64</td>\n",
       "      <td>9.899999</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.63</td>\n",
       "      <td>470.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>14.16</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.48</td>\n",
       "      <td>20.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.24</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.71</td>\n",
       "      <td>660.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0     12.86        1.35  2.32               18.0      122.0           1.51   \n",
       "1     12.88        2.99  2.40               20.0      104.0           1.30   \n",
       "2     12.81        2.31  2.40               24.0       98.0           1.15   \n",
       "3     12.70        3.55  2.36               21.5      106.0           1.70   \n",
       "4     12.51        1.24  2.25               17.5       85.0           2.00   \n",
       "5     12.60        2.46  2.20               18.5       94.0           1.62   \n",
       "6     12.25        4.72  2.54               21.0       89.0           1.38   \n",
       "7     12.53        5.51  2.64               25.0       96.0           1.79   \n",
       "8     13.49        3.59  2.19               19.5       88.0           1.62   \n",
       "9     12.84        2.96  2.61               24.0      101.0           2.32   \n",
       "10    12.93        2.81  2.70               21.0       96.0           1.54   \n",
       "11    13.36        2.56  2.35               20.0       89.0           1.40   \n",
       "12    13.52        3.17  2.72               23.5       97.0           1.55   \n",
       "13    13.62        4.95  2.35               20.0       92.0           2.00   \n",
       "14    12.25        3.88  2.20               18.5      112.0           1.38   \n",
       "15    13.16        3.57  2.15               21.0      102.0           1.50   \n",
       "16    13.88        5.04  2.23               20.0       80.0           0.98   \n",
       "17    12.87        4.61  2.48               21.5       86.0           1.70   \n",
       "18    13.32        3.24  2.38               21.5       92.0           1.93   \n",
       "19    13.08        3.90  2.36               21.5      113.0           1.41   \n",
       "20    13.50        3.12  2.62               24.0      123.0           1.40   \n",
       "21    12.79        2.67  2.48               22.0      112.0           1.48   \n",
       "22    13.11        1.90  2.75               25.5      116.0           2.20   \n",
       "23    13.23        3.30  2.28               18.5       98.0           1.80   \n",
       "24    12.58        1.29  2.10               20.0      103.0           1.48   \n",
       "25    13.17        5.19  2.32               22.0       93.0           1.74   \n",
       "26    13.84        4.12  2.38               19.5       89.0           1.80   \n",
       "27    12.45        3.03  2.64               27.0       97.0           1.90   \n",
       "28    14.34        1.68  2.70               25.0       98.0           2.80   \n",
       "29    13.48        1.67  2.64               22.5       89.0           2.60   \n",
       "30    12.36        3.83  2.38               21.0       88.0           2.30   \n",
       "31    13.69        3.26  2.54               20.0      107.0           1.83   \n",
       "32    12.85        3.27  2.58               22.0      106.0           1.65   \n",
       "33    12.96        3.45  2.35               18.5      106.0           1.39   \n",
       "34    13.78        2.76  2.30               22.0       90.0           1.35   \n",
       "35    13.73        4.36  2.26               22.5       88.0           1.28   \n",
       "36    13.45        3.70  2.60               23.0      111.0           1.70   \n",
       "37    12.82        3.37  2.30               19.5       88.0           1.48   \n",
       "38    13.58        2.58  2.69               24.5      105.0           1.55   \n",
       "39    13.40        4.60  2.86               25.0      112.0           1.98   \n",
       "40    12.20        3.03  2.32               19.0       96.0           1.25   \n",
       "41    12.77        2.39  2.28               19.5       86.0           1.39   \n",
       "42    14.16        2.51  2.48               20.0       91.0           1.68   \n",
       "43    13.71        5.65  2.45               20.5       95.0           1.68   \n",
       "44    13.40        3.91  2.48               23.0      102.0           1.80   \n",
       "45    13.27        4.28  2.26               20.0      120.0           1.59   \n",
       "46    13.17        2.59  2.37               20.0      120.0           1.65   \n",
       "47    14.13        4.10  2.74               24.5       96.0           2.05   \n",
       "\n",
       "    flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0         1.25                  0.21             0.94         4.100000  0.76   \n",
       "1         1.22                  0.24             0.83         5.400000  0.74   \n",
       "2         1.09                  0.27             0.83         5.700000  0.66   \n",
       "3         1.20                  0.17             0.84         5.000000  0.78   \n",
       "4         0.58                  0.60             1.25         5.450000  0.75   \n",
       "5         0.66                  0.63             0.94         7.100000  0.73   \n",
       "6         0.47                  0.53             0.80         3.850000  0.75   \n",
       "7         0.60                  0.63             1.10         5.000000  0.82   \n",
       "8         0.48                  0.58             0.88         5.700000  0.81   \n",
       "9         0.60                  0.53             0.81         4.920000  0.89   \n",
       "10        0.50                  0.53             0.75         4.600000  0.77   \n",
       "11        0.50                  0.37             0.64         5.600000  0.70   \n",
       "12        0.52                  0.50             0.55         4.350000  0.89   \n",
       "13        0.80                  0.47             1.02         4.400000  0.91   \n",
       "14        0.78                  0.29             1.14         8.210000  0.65   \n",
       "15        0.55                  0.43             1.30         4.000000  0.60   \n",
       "16        0.34                  0.40             0.68         4.900000  0.58   \n",
       "17        0.65                  0.47             0.86         7.650000  0.54   \n",
       "18        0.76                  0.45             1.25         8.420000  0.55   \n",
       "19        1.39                  0.34             1.14         9.400000  0.57   \n",
       "20        1.57                  0.22             1.25         8.600000  0.59   \n",
       "21        1.36                  0.24             1.26        10.800000  0.48   \n",
       "22        1.28                  0.26             1.56         7.100000  0.61   \n",
       "23        0.83                  0.61             1.87        10.520000  0.56   \n",
       "24        0.58                  0.53             1.40         7.600000  0.58   \n",
       "25        0.63                  0.61             1.55         7.900000  0.60   \n",
       "26        0.83                  0.48             1.56         9.010000  0.57   \n",
       "27        0.58                  0.63             1.14         7.500000  0.67   \n",
       "28        1.31                  0.53             2.70        13.000000  0.57   \n",
       "29        1.10                  0.52             2.29        11.750000  0.57   \n",
       "30        0.92                  0.50             1.04         7.650000  0.56   \n",
       "31        0.56                  0.50             0.80         5.880000  0.96   \n",
       "32        0.60                  0.60             0.96         5.580000  0.87   \n",
       "33        0.70                  0.40             0.94         5.280000  0.68   \n",
       "34        0.68                  0.41             1.03         9.580000  0.70   \n",
       "35        0.47                  0.52             1.15         6.620000  0.78   \n",
       "36        0.92                  0.43             1.46        10.680000  0.85   \n",
       "37        0.66                  0.40             0.97        10.260000  0.72   \n",
       "38        0.84                  0.39             1.54         8.660000  0.74   \n",
       "39        0.96                  0.27             1.11         8.500000  0.67   \n",
       "40        0.49                  0.40             0.73         5.500000  0.66   \n",
       "41        0.51                  0.48             0.64         9.899999  0.57   \n",
       "42        0.70                  0.44             1.24         9.700000  0.62   \n",
       "43        0.61                  0.52             1.06         7.700000  0.64   \n",
       "44        0.75                  0.43             1.41         7.300000  0.70   \n",
       "45        0.69                  0.43             1.35        10.200000  0.59   \n",
       "46        0.68                  0.53             1.46         9.300000  0.60   \n",
       "47        0.76                  0.56             1.35         9.200000  0.61   \n",
       "\n",
       "    od280/od315_of_diluted_wines  proline  label  \n",
       "0                           1.29    630.0      2  \n",
       "1                           1.42    530.0      2  \n",
       "2                           1.36    560.0      2  \n",
       "3                           1.29    600.0      2  \n",
       "4                           1.51    650.0      2  \n",
       "5                           1.58    695.0      2  \n",
       "6                           1.27    720.0      2  \n",
       "7                           1.69    515.0      2  \n",
       "8                           1.82    580.0      2  \n",
       "9                           2.15    590.0      2  \n",
       "10                          2.31    600.0      2  \n",
       "11                          2.47    780.0      2  \n",
       "12                          2.06    520.0      2  \n",
       "13                          2.05    550.0      2  \n",
       "14                          2.00    855.0      2  \n",
       "15                          1.68    830.0      2  \n",
       "16                          1.33    415.0      2  \n",
       "17                          1.86    625.0      2  \n",
       "18                          1.62    650.0      2  \n",
       "19                          1.33    550.0      2  \n",
       "20                          1.30    500.0      2  \n",
       "21                          1.47    480.0      2  \n",
       "22                          1.33    425.0      2  \n",
       "23                          1.51    675.0      2  \n",
       "24                          1.55    640.0      2  \n",
       "25                          1.48    725.0      2  \n",
       "26                          1.64    480.0      2  \n",
       "27                          1.73    880.0      2  \n",
       "28                          1.96    660.0      2  \n",
       "29                          1.78    620.0      2  \n",
       "30                          1.58    520.0      2  \n",
       "31                          1.82    680.0      2  \n",
       "32                          2.11    570.0      2  \n",
       "33                          1.75    675.0      2  \n",
       "34                          1.68    615.0      2  \n",
       "35                          1.75    520.0      2  \n",
       "36                          1.56    695.0      2  \n",
       "37                          1.75    685.0      2  \n",
       "38                          1.80    750.0      2  \n",
       "39                          1.92    630.0      2  \n",
       "40                          1.83    510.0      2  \n",
       "41                          1.63    470.0      2  \n",
       "42                          1.71    660.0      2  \n",
       "43                          1.74    740.0      2  \n",
       "44                          1.56    750.0      2  \n",
       "45                          1.56    835.0      2  \n",
       "46                          1.62    840.0      2  \n",
       "47                          1.60    560.0      2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_2 = wine_data.data[130:]\n",
    "wine_t_2 = wine_data.target[130:]\n",
    "wine_df_2 = pd.DataFrame(data = wine_2, columns = wine_feature)\n",
    "wine_df_2[\"label\"] = wine_t_2\n",
    "wine_df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf307be",
   "metadata": {},
   "source": [
    "### 이렇게 데이터를 3개로 나누니 데이터 분포가 0번은 평균이었고 1번은 많고 2번은 데이터의 양이 적었다.\n",
    "\n",
    "# 과연 이중 어떤 특징이 와인의 종류를 정하는데 중요한지 궁금해\n",
    "\n",
    "# 3개로 나눈 데이터 표에서 각 특징들의 평균, 표준편차를 구하고\n",
    "\n",
    "# 그 결과로 어떤 특징이 종류를 정하는데 중요한지 볼 예정이다\n",
    "\n",
    "# 표준편차가 크면 클수록 데이터 분포 범위가 넓어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff571dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소값\n",
      "alcohol                          12.85\n",
      "malic_acid                        1.35\n",
      "ash                               2.04\n",
      "alcalinity_of_ash                11.20\n",
      "magnesium                        89.00\n",
      "total_phenols                     2.20\n",
      "flavanoids                        2.19\n",
      "nonflavanoid_phenols              0.17\n",
      "proanthocyanins                   1.25\n",
      "color_intensity                   3.52\n",
      "hue                               0.82\n",
      "od280/od315_of_diluted_wines      2.51\n",
      "proline                         680.00\n",
      "label                             0.00\n",
      "dtype: float64\n",
      "\n",
      "최대값\n",
      "alcohol                           14.83\n",
      "malic_acid                         4.04\n",
      "ash                                3.22\n",
      "alcalinity_of_ash                 25.00\n",
      "magnesium                        132.00\n",
      "total_phenols                      3.88\n",
      "flavanoids                         3.93\n",
      "nonflavanoid_phenols               0.50\n",
      "proanthocyanins                    2.96\n",
      "color_intensity                    8.90\n",
      "hue                                1.28\n",
      "od280/od315_of_diluted_wines       4.00\n",
      "proline                         1680.00\n",
      "label                              0.00\n",
      "dtype: float64\n",
      "\n",
      "평균\n",
      "alcohol                           13.744746\n",
      "malic_acid                         2.010678\n",
      "ash                                2.455593\n",
      "alcalinity_of_ash                 17.037288\n",
      "magnesium                        106.338983\n",
      "total_phenols                      2.840169\n",
      "flavanoids                         2.982373\n",
      "nonflavanoid_phenols               0.290000\n",
      "proanthocyanins                    1.899322\n",
      "color_intensity                    5.528305\n",
      "hue                                1.062034\n",
      "od280/od315_of_diluted_wines       3.157797\n",
      "proline                         1115.711864\n",
      "label                              0.000000\n",
      "dtype: float64\n",
      "\n",
      "표준편차\n",
      "alcohol                           0.462125\n",
      "malic_acid                        0.688549\n",
      "ash                               0.227166\n",
      "alcalinity_of_ash                 2.546322\n",
      "magnesium                        10.498949\n",
      "total_phenols                     0.338961\n",
      "flavanoids                        0.397494\n",
      "nonflavanoid_phenols              0.070049\n",
      "proanthocyanins                   0.412109\n",
      "color_intensity                   1.238573\n",
      "hue                               0.116483\n",
      "od280/od315_of_diluted_wines      0.357077\n",
      "proline                         221.520767\n",
      "label                             0.000000\n",
      "dtype: float64\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mins = wine_df_0.min()\n",
    "maxs = wine_df_0.max()\n",
    "print('최소값')\n",
    "print(mins)\n",
    "print()\n",
    "print('최대값')\n",
    "print(maxs)\n",
    "print()\n",
    "print('평균')\n",
    "print(wine_df_0.mean())\n",
    "print()\n",
    "print('표준편차')\n",
    "print(wine_df_0.std())\n",
    "a = list(wine_df_0.std())\n",
    "print(a.sort())\n",
    "a\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dcd3a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소값\n",
      "alcohol                          11.03\n",
      "malic_acid                        0.74\n",
      "ash                               1.36\n",
      "alcalinity_of_ash                10.60\n",
      "magnesium                        70.00\n",
      "total_phenols                     1.10\n",
      "flavanoids                        0.57\n",
      "nonflavanoid_phenols              0.13\n",
      "proanthocyanins                   0.41\n",
      "color_intensity                   1.28\n",
      "hue                               0.69\n",
      "od280/od315_of_diluted_wines      1.59\n",
      "proline                         278.00\n",
      "label                             1.00\n",
      "dtype: float64\n",
      "\n",
      "최대값\n",
      "alcohol                          13.86\n",
      "malic_acid                        5.80\n",
      "ash                               3.23\n",
      "alcalinity_of_ash                30.00\n",
      "magnesium                       162.00\n",
      "total_phenols                     3.52\n",
      "flavanoids                        5.08\n",
      "nonflavanoid_phenols              0.66\n",
      "proanthocyanins                   3.58\n",
      "color_intensity                   6.00\n",
      "hue                               1.71\n",
      "od280/od315_of_diluted_wines      3.69\n",
      "proline                         985.00\n",
      "label                             1.00\n",
      "dtype: float64\n",
      "\n",
      "평균\n",
      "alcohol                          12.278732\n",
      "malic_acid                        1.932676\n",
      "ash                               2.244789\n",
      "alcalinity_of_ash                20.238028\n",
      "magnesium                        94.549296\n",
      "total_phenols                     2.258873\n",
      "flavanoids                        2.080845\n",
      "nonflavanoid_phenols              0.363662\n",
      "proanthocyanins                   1.630282\n",
      "color_intensity                   3.086620\n",
      "hue                               1.056282\n",
      "od280/od315_of_diluted_wines      2.785352\n",
      "proline                         519.507042\n",
      "label                             1.000000\n",
      "dtype: float64\n",
      "\n",
      "표준편차\n",
      "alcohol                           0.537964\n",
      "malic_acid                        1.015569\n",
      "ash                               0.315467\n",
      "alcalinity_of_ash                 3.349770\n",
      "magnesium                        16.753497\n",
      "total_phenols                     0.545361\n",
      "flavanoids                        0.705701\n",
      "nonflavanoid_phenols              0.123961\n",
      "proanthocyanins                   0.602068\n",
      "color_intensity                   0.924929\n",
      "hue                               0.202937\n",
      "od280/od315_of_diluted_wines      0.496573\n",
      "proline                         157.211220\n",
      "label                             0.000000\n",
      "dtype: float64\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.12396127778601694,\n",
       " 0.20293680811654444,\n",
       " 0.31546726864369473,\n",
       " 0.4965734904204311,\n",
       " 0.5379642302964676,\n",
       " 0.5453610843043734,\n",
       " 0.6020677983440211,\n",
       " 0.7057007590815028,\n",
       " 0.9249292539153958,\n",
       " 1.0155687467009085,\n",
       " 3.3497704056582367,\n",
       " 16.753497487641514,\n",
       " 157.21122035923852]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mins = wine_df_1.min()\n",
    "maxs = wine_df_1.max()\n",
    "print('최소값')\n",
    "print(mins)\n",
    "print()\n",
    "print('최대값')\n",
    "print(maxs)\n",
    "print()\n",
    "print('평균')\n",
    "print(wine_df_1.mean())\n",
    "print()\n",
    "print('표준편차')\n",
    "print(wine_df_1.std())\n",
    "a = list(wine_df_1.std())\n",
    "print(a.sort())\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d31d80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소값\n",
      "alcohol                          12.20\n",
      "malic_acid                        1.24\n",
      "ash                               2.10\n",
      "alcalinity_of_ash                17.50\n",
      "magnesium                        80.00\n",
      "total_phenols                     0.98\n",
      "flavanoids                        0.34\n",
      "nonflavanoid_phenols              0.17\n",
      "proanthocyanins                   0.55\n",
      "color_intensity                   3.85\n",
      "hue                               0.48\n",
      "od280/od315_of_diluted_wines      1.27\n",
      "proline                         415.00\n",
      "label                             2.00\n",
      "dtype: float64\n",
      "\n",
      "최대값\n",
      "alcohol                          14.34\n",
      "malic_acid                        5.65\n",
      "ash                               2.86\n",
      "alcalinity_of_ash                27.00\n",
      "magnesium                       123.00\n",
      "total_phenols                     2.80\n",
      "flavanoids                        1.57\n",
      "nonflavanoid_phenols              0.63\n",
      "proanthocyanins                   2.70\n",
      "color_intensity                  13.00\n",
      "hue                               0.96\n",
      "od280/od315_of_diluted_wines      2.47\n",
      "proline                         880.00\n",
      "label                             2.00\n",
      "dtype: float64\n",
      "\n",
      "평균\n",
      "alcohol                          13.153750\n",
      "malic_acid                        3.333750\n",
      "ash                               2.437083\n",
      "alcalinity_of_ash                21.416667\n",
      "magnesium                        99.312500\n",
      "total_phenols                     1.678750\n",
      "flavanoids                        0.781458\n",
      "nonflavanoid_phenols              0.447500\n",
      "proanthocyanins                   1.153542\n",
      "color_intensity                   7.396250\n",
      "hue                               0.682708\n",
      "od280/od315_of_diluted_wines      1.683542\n",
      "proline                         629.895833\n",
      "label                             2.000000\n",
      "dtype: float64\n",
      "\n",
      "표준편차\n",
      "alcohol                           0.530241\n",
      "malic_acid                        1.087906\n",
      "ash                               0.184690\n",
      "alcalinity_of_ash                 2.258161\n",
      "magnesium                        10.890473\n",
      "total_phenols                     0.356971\n",
      "flavanoids                        0.293504\n",
      "nonflavanoid_phenols              0.124140\n",
      "proanthocyanins                   0.408836\n",
      "color_intensity                   2.310942\n",
      "hue                               0.114441\n",
      "od280/od315_of_diluted_wines      0.272111\n",
      "proline                         115.097043\n",
      "label                             0.000000\n",
      "dtype: float64\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.11444109482348182,\n",
       " 0.12413959198367105,\n",
       " 0.1846901756805375,\n",
       " 0.2721114413706684,\n",
       " 0.293504065601863,\n",
       " 0.3569708552380176,\n",
       " 0.408835864601428,\n",
       " 0.5302413139918748,\n",
       " 1.0879057081324806,\n",
       " 2.2581609287519444,\n",
       " 2.3109421491810918,\n",
       " 10.890472607606213,\n",
       " 115.09704315911695]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mins = wine_df_2.min()\n",
    "maxs = wine_df_2.max()\n",
    "print('최소값')\n",
    "print(mins)\n",
    "print()\n",
    "print('최대값')\n",
    "print(maxs)\n",
    "print()\n",
    "print('평균')\n",
    "print(wine_df_2.mean())\n",
    "print()\n",
    "print('표준편차')\n",
    "print(wine_df_2.std())\n",
    "a = list(wine_df_2.std())\n",
    "print(a.sort())\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdf6fb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소값\n",
      "alcohol                          11.03\n",
      "malic_acid                        0.74\n",
      "ash                               1.36\n",
      "alcalinity_of_ash                10.60\n",
      "magnesium                        70.00\n",
      "total_phenols                     0.98\n",
      "flavanoids                        0.34\n",
      "nonflavanoid_phenols              0.13\n",
      "proanthocyanins                   0.41\n",
      "color_intensity                   1.28\n",
      "hue                               0.48\n",
      "od280/od315_of_diluted_wines      1.27\n",
      "proline                         278.00\n",
      "label                             0.00\n",
      "dtype: float64\n",
      "\n",
      "최대값\n",
      "alcohol                           14.83\n",
      "malic_acid                         5.80\n",
      "ash                                3.23\n",
      "alcalinity_of_ash                 30.00\n",
      "magnesium                        162.00\n",
      "total_phenols                      3.88\n",
      "flavanoids                         5.08\n",
      "nonflavanoid_phenols               0.66\n",
      "proanthocyanins                    3.58\n",
      "color_intensity                   13.00\n",
      "hue                                1.71\n",
      "od280/od315_of_diluted_wines       4.00\n",
      "proline                         1680.00\n",
      "label                              2.00\n",
      "dtype: float64\n",
      "\n",
      "평균\n",
      "alcohol                          13.000618\n",
      "malic_acid                        2.336348\n",
      "ash                               2.366517\n",
      "alcalinity_of_ash                19.494944\n",
      "magnesium                        99.741573\n",
      "total_phenols                     2.295112\n",
      "flavanoids                        2.029270\n",
      "nonflavanoid_phenols              0.361854\n",
      "proanthocyanins                   1.590899\n",
      "color_intensity                   5.058090\n",
      "hue                               0.957449\n",
      "od280/od315_of_diluted_wines      2.611685\n",
      "proline                         746.893258\n",
      "label                             0.938202\n",
      "dtype: float64\n",
      "\n",
      "표준편차\n",
      "alcohol                           0.811827\n",
      "malic_acid                        1.117146\n",
      "ash                               0.274344\n",
      "alcalinity_of_ash                 3.339564\n",
      "magnesium                        14.282484\n",
      "total_phenols                     0.625851\n",
      "flavanoids                        0.998859\n",
      "nonflavanoid_phenols              0.124453\n",
      "proanthocyanins                   0.572359\n",
      "color_intensity                   2.318286\n",
      "hue                               0.228572\n",
      "od280/od315_of_diluted_wines      0.709990\n",
      "proline                         314.907474\n",
      "label                             0.775035\n",
      "dtype: float64\n",
      "alcohol\n"
     ]
    }
   ],
   "source": [
    "mins = wine_df.min()\n",
    "maxs = wine_df.max()\n",
    "print('최소값')\n",
    "print(mins)\n",
    "print()\n",
    "print('최대값')\n",
    "print(maxs)\n",
    "print()\n",
    "print('평균')\n",
    "print(wine_df.mean())\n",
    "print()\n",
    "print('표준편차')\n",
    "print(wine_df.std())\n",
    "print(wine_df.columns[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21ec8f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8118265380058577,\n",
       " 1.1171460976144627,\n",
       " 0.2743440090608148,\n",
       " 3.3395637671735052,\n",
       " 14.282483515295668,\n",
       " 0.6258510488339891,\n",
       " 0.9988586850169465,\n",
       " 0.12445334029667939,\n",
       " 0.5723588626747611,\n",
       " 2.318285871822413,\n",
       " 0.22857156582982338,\n",
       " 0.7099904287650505,\n",
       " 314.9074742768489,\n",
       " 0.7750349899850565]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(wine_df.std())\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15000358",
   "metadata": {},
   "source": [
    "# 각 표의 표준편차를 크기순으로 보고 0~1값들을 중요 특성이라고 분류했다.\n",
    "\n",
    "#### 클래스 0 : non_flavnoid, hue, ash, total_phenols, od280/od315, flavnoid, proanthocyanins, alchol, malic_acid\n",
    "\n",
    "#### 클래스 1 : non_flavnoid, hue, ash, od280/od315, alchol, total_phenols, proanthocyanins, flavnoid, color_intensity\n",
    "\n",
    "#### 클래스 2 : hue, non_flavnoid, ash, od280/od315, flavnoid, total_phenols, proanthocyanins, alchol\n",
    "\n",
    "### 판다스 데이터 정렬 후  iloc함수를 사용해 데이터를 추출 슬라이스로 데이터를 추릴것이다.\n",
    "\n",
    "### 먼저 데이터를 하위 특성들을 기준으로 정렬해 일정 하위와 상위를 제거한다.\n",
    "\n",
    "### 그리고 이 과정이 2번이상이면 각 과정에서 나온 데이터중 중복 데이터를 제거한다.\n",
    "\n",
    "### 이때 제거 방법은 수학의 집합 공식을 이용해 제거한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0843ad11",
   "metadata": {},
   "source": [
    "# 1&2. 데이터 특정 컬럼 기준으로 정렬, 자르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd7ffc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_temp = wine_df_0.copy()\n",
    "wine_t_0 = wine_temp.sort_values(by = [wine_temp.columns[0], wine_temp.columns[1]])\n",
    "wine_tf_0 = wine_t_0[3:56]\n",
    "wine_temp_1 = wine_df_1.copy()\n",
    "wine_t_1 = wine_temp_1.sort_values(by = [wine_temp_1.columns[6], wine_temp_1.columns[9]])\n",
    "wine_tf_1 = wine_t_1[3:68]\n",
    "wine_temp_2 = wine_df_2.copy()\n",
    "wine_t_2 = wine_temp_2.sort_values(by = [wine_temp_2.columns[0], wine_temp_2.columns[8]])\n",
    "wine_tf_2 = wine_t_2[2:46]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad6e8c0",
   "metadata": {},
   "source": [
    "#  3. 중복 확인 후 중복 제거(할려고 했으나 그러면 너무 빡세서... 나중에 할게요~~)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a5846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ab37910",
   "metadata": {},
   "source": [
    "# 4. 각 종류별로 전처리한 데이터 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c727f131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>13.05</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.04</td>\n",
       "      <td>12.4</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2.72</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.17</td>\n",
       "      <td>2.91</td>\n",
       "      <td>7.20</td>\n",
       "      <td>1.12</td>\n",
       "      <td>2.91</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>13.05</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.10</td>\n",
       "      <td>17.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.03</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.88</td>\n",
       "      <td>3.35</td>\n",
       "      <td>885.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13.05</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.22</td>\n",
       "      <td>25.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1.13</td>\n",
       "      <td>3.20</td>\n",
       "      <td>830.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>13.07</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.10</td>\n",
       "      <td>15.5</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.37</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.18</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>13.73</td>\n",
       "      <td>4.36</td>\n",
       "      <td>2.26</td>\n",
       "      <td>22.5</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.15</td>\n",
       "      <td>6.62</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.75</td>\n",
       "      <td>520.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>13.78</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.30</td>\n",
       "      <td>22.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.03</td>\n",
       "      <td>9.58</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.68</td>\n",
       "      <td>615.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13.84</td>\n",
       "      <td>4.12</td>\n",
       "      <td>2.38</td>\n",
       "      <td>19.5</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.56</td>\n",
       "      <td>9.01</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.64</td>\n",
       "      <td>480.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13.88</td>\n",
       "      <td>5.04</td>\n",
       "      <td>2.23</td>\n",
       "      <td>20.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.68</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.33</td>\n",
       "      <td>415.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "50    13.05        1.73  2.04               12.4       92.0           2.72   \n",
       "44    13.05        1.77  2.10               17.0      107.0           3.00   \n",
       "25    13.05        2.05  3.22               25.0      124.0           2.63   \n",
       "38    13.07        1.50  2.10               15.5       98.0           2.40   \n",
       "2     13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "..      ...         ...   ...                ...        ...            ...   \n",
       "35    13.73        4.36  2.26               22.5       88.0           1.28   \n",
       "34    13.78        2.76  2.30               22.0       90.0           1.35   \n",
       "26    13.84        4.12  2.38               19.5       89.0           1.80   \n",
       "16    13.88        5.04  2.23               20.0       80.0           0.98   \n",
       "47    14.13        4.10  2.74               24.5       96.0           2.05   \n",
       "\n",
       "    flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "50        3.27                  0.17             2.91             7.20  1.12   \n",
       "44        3.00                  0.28             2.03             5.04  0.88   \n",
       "25        2.68                  0.47             1.92             3.58  1.13   \n",
       "38        2.64                  0.28             1.37             3.70  1.18   \n",
       "2         3.24                  0.30             2.81             5.68  1.03   \n",
       "..         ...                   ...              ...              ...   ...   \n",
       "35        0.47                  0.52             1.15             6.62  0.78   \n",
       "34        0.68                  0.41             1.03             9.58  0.70   \n",
       "26        0.83                  0.48             1.56             9.01  0.57   \n",
       "16        0.34                  0.40             0.68             4.90  0.58   \n",
       "47        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "    od280/od315_of_diluted_wines  proline  label  \n",
       "50                          2.91   1150.0      0  \n",
       "44                          3.35    885.0      0  \n",
       "25                          3.20    830.0      0  \n",
       "38                          2.69   1020.0      0  \n",
       "2                           3.17   1185.0      0  \n",
       "..                           ...      ...    ...  \n",
       "35                          1.75    520.0      2  \n",
       "34                          1.68    615.0      2  \n",
       "26                          1.64    480.0      2  \n",
       "16                          1.33    415.0      2  \n",
       "47                          1.60    560.0      2  \n",
       "\n",
       "[162 rows x 14 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_sum_data = pd.concat([wine_tf_0, wine_tf_1, wine_tf_2])\n",
    "wine_sum_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cd0417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data = load_wine()\n",
    "wine_tlabel = wine_data.target.copy()\n",
    "wine_label_0 = wine_tlabel[:59]\n",
    "wine_label_1 = wine_tlabel[59:130]\n",
    "wine_label_2 = wine_tlabel[130:]\n",
    "wine_labelf_0 = wine_label_0[3:56]\n",
    "wine_labelf_1 = wine_label_1[3:68]\n",
    "wine_labelf_2 = wine_label_2[2:46]\n",
    "wine_label_sum = []\n",
    "wine_label_sum.extend(wine_labelf_0)\n",
    "wine_label_sum.extend(wine_labelf_1)\n",
    "wine_label_sum.extend(wine_labelf_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def860b",
   "metadata": {},
   "source": [
    "# 5. 각 모델로 머신러닝 돌리기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ae595",
   "metadata": {},
   "source": [
    "# 5-1. 기본 데이터로 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e94e28a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.88        31\n",
      "           1       0.50      0.90      0.64        31\n",
      "           2       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.63        89\n",
      "   macro avg       0.45      0.60      0.51        89\n",
      "weighted avg       0.47      0.63      0.53        89\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.77      0.86        31\n",
      "           1       1.00      0.16      0.28        31\n",
      "           2       0.44      0.96      0.60        27\n",
      "\n",
      "    accuracy                           0.62        89\n",
      "   macro avg       0.80      0.63      0.58        89\n",
      "weighted avg       0.82      0.62      0.58        89\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\samsung\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\samsung\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "svm_model = svm.SVC()\n",
    "sgd_model = SGDClassifier()\n",
    "\n",
    "\n",
    "wine_data = load_wine()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine_data.data, \n",
    "                                                    wine_data.target, \n",
    "                                                    test_size=0.5, \n",
    "                                                    random_state=random.randrange(20,30))\n",
    "\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "sgd_model.fit(X_train, y_train)\n",
    "y_pred_sgd = sgd_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a2d7a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91        30\n",
      "           1       0.85      0.91      0.88        32\n",
      "           2       0.90      1.00      0.95        27\n",
      "\n",
      "    accuracy                           0.91        89\n",
      "   macro avg       0.92      0.91      0.91        89\n",
      "weighted avg       0.92      0.91      0.91        89\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        30\n",
      "           1       0.97      0.94      0.95        32\n",
      "           2       0.93      1.00      0.96        27\n",
      "\n",
      "    accuracy                           0.97        89\n",
      "   macro avg       0.97      0.97      0.97        89\n",
      "weighted avg       0.97      0.97      0.97        89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(random_state=random.randrange(10,30))\n",
    "random_forest = RandomForestClassifier(random_state=random.randrange(10,30))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine_data.data, \n",
    "                                                    wine_data.target, \n",
    "                                                    test_size=0.5, \n",
    "                                                    random_state=random.randrange(0,40))\n",
    "\n",
    "\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred_de = decision_tree.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_de))\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred_rf = random_forest.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544953db",
   "metadata": {},
   "source": [
    "# 5-2. 전처리 데이터로 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e57286df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        25\n",
      "           1       1.00      1.00      1.00        34\n",
      "           2       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           1.00        81\n",
      "   macro avg       1.00      1.00      1.00        81\n",
      "weighted avg       1.00      1.00      1.00        81\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        25\n",
      "           1       1.00      0.97      0.99        34\n",
      "           2       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           0.99        81\n",
      "   macro avg       0.99      0.99      0.99        81\n",
      "weighted avg       0.99      0.99      0.99        81\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.86        25\n",
      "           1       0.67      0.65      0.66        34\n",
      "           2       0.41      0.41      0.41        22\n",
      "\n",
      "    accuracy                           0.65        81\n",
      "   macro avg       0.64      0.65      0.64        81\n",
      "weighted avg       0.65      0.65      0.65        81\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      1.00      0.53        25\n",
      "           1       0.92      0.32      0.48        34\n",
      "           2       0.00      0.00      0.00        22\n",
      "\n",
      "    accuracy                           0.44        81\n",
      "   macro avg       0.43      0.44      0.34        81\n",
      "weighted avg       0.50      0.44      0.36        81\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\samsung\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\samsung\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(random_state=random.randrange(30,70))\n",
    "random_forest = RandomForestClassifier(random_state=random.randrange(40,80))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine_sum_data, \n",
    "                                                    wine_label_sum, \n",
    "                                                    test_size=0.5, \n",
    "                                                    random_state=random.randrange(20,100))\n",
    "\n",
    "\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred_de = decision_tree.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_de))\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred_rf = random_forest.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "sgd_model.fit(X_train, y_train)\n",
    "y_pred_sgd = sgd_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21068d6e",
   "metadata": {},
   "source": [
    "# 6. 결론 \n",
    "\n",
    "## 데이터 갯수가 200개 미만이다보니 일부 모델은 학습량이 적어 낮은 성능을 보였다.\n",
    "\n",
    "## 의사결정트리, 랜덤포레스트는 좋은 성능을 보였으나\n",
    "\n",
    "## svm sgd은 성능이 영 좋지 못했다.\n",
    "\n",
    "## 데이터 전처리는 좋은 시도인것 같았지만\n",
    "\n",
    "## 기존 데이터 양이 적어 앞서 말한 svm,sgd은 학습데이터 수가 더 줄어 성능이 더 낮아졌다..\n",
    "\n",
    "#### 데이터 수가 적을 때는 전처리를 조금만 해야 하며, 의사결정트리, 랜덤포레스트가 성능이 더 좋다는 점을 깨달았다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a70227",
   "metadata": {},
   "source": [
    "# 유방암 악성,양성 종양 판단하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30624aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f15be35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d36501d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8423eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malignant', 'benign'], dtype='<U9')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85879f18",
   "metadata": {},
   "source": [
    "## mean_radius - 평균 반지름\n",
    "\n",
    "## mean_texture - 평균 질감\n",
    "\n",
    "## mean_perimeter - 평균 둘레\n",
    "\n",
    "## mean_area - 평균 넓이\n",
    "\n",
    "## mean_smoothness - 평균 매끈함\n",
    "\n",
    "## mean_compactness - 평균 밀집도\n",
    "\n",
    "## mean_concavity - 평균 오목함\n",
    "\n",
    "## mean_concave_points - 평균 오목한 점(위치)\n",
    "\n",
    "## mean_symmetry - 평균 대칭\n",
    "\n",
    "## mean_fractal_dimension - 평균 프랙탈 차원\n",
    "\n",
    "## radius_error - 반지름 오차값\n",
    "\n",
    "## texture_error - 질감 오차값\n",
    "\n",
    "##          ....  ~오차 평균 값\n",
    "\n",
    "## worst_radius - 아픈상태에서의 평균 반지름\n",
    "\n",
    "##          .... ~ 아플때 평균 값\n",
    "\n",
    "## 즉 특징은 반지름, 질감, 둘레, 넓이, 매끈함, 밀집도, 오목함, 오목한 위치, 대칭, 프랙탈 차원이고\n",
    "\n",
    "## 이 종류는 정상, 아플때, 오차값이고 3 * 10 = 30가지의 컬럼을 갖는다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ba32f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  label  \n",
       "0                  0.2654          0.4601                  0.11890      0  \n",
       "1                  0.1860          0.2750                  0.08902      0  \n",
       "2                  0.2430          0.3613                  0.08758      0  \n",
       "3                  0.2575          0.6638                  0.17300      0  \n",
       "4                  0.1625          0.2364                  0.07678      0  \n",
       "..                    ...             ...                      ...    ...  \n",
       "564                0.2216          0.2060                  0.07115      0  \n",
       "565                0.1628          0.2572                  0.06637      0  \n",
       "566                0.1418          0.2218                  0.07820      0  \n",
       "567                0.2650          0.4087                  0.12400      0  \n",
       "568                0.0000          0.2871                  0.07039      1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cancer_df = pd.DataFrame(data = cancer.data, columns = cancer.feature_names)\n",
    "cancer_df[\"label\"] = cancer.target\n",
    "cancer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e6a5838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "mean radius                  17.990000\n",
      "mean texture                 10.380000\n",
      "mean perimeter              122.800000\n",
      "mean area                  1001.000000\n",
      "mean smoothness               0.118400\n",
      "mean compactness              0.277600\n",
      "mean concavity                0.300100\n",
      "mean concave points           0.147100\n",
      "mean symmetry                 0.241900\n",
      "mean fractal dimension        0.078710\n",
      "radius error                  1.095000\n",
      "texture error                 0.905300\n",
      "perimeter error               8.589000\n",
      "area error                  153.400000\n",
      "smoothness error              0.006399\n",
      "compactness error             0.049040\n",
      "concavity error               0.053730\n",
      "concave points error          0.015870\n",
      "symmetry error                0.030030\n",
      "fractal dimension error       0.006193\n",
      "worst radius                 25.380000\n",
      "worst texture                17.330000\n",
      "worst perimeter             184.600000\n",
      "worst area                 2019.000000\n",
      "worst smoothness              0.162200\n",
      "worst compactness             0.665600\n",
      "worst concavity               0.711900\n",
      "worst concave points          0.265400\n",
      "worst symmetry                0.460100\n",
      "worst fractal dimension       0.118900\n",
      "label                         0.000000\n",
      "Name: 0, dtype: float64\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error']\n"
     ]
    }
   ],
   "source": [
    "print(cancer.target[568])\n",
    "print(cancer_df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc26bc",
   "metadata": {},
   "source": [
    "# 데이터가 한 사람당 정상일때 아플때 오차값으로 이루어져 있다.\n",
    "\n",
    "# 아플때 모든 값이 정상일때 값보다 크거나 같다.\n",
    "\n",
    "# 양성, 음성도 둘다 위 조건은 같다.\n",
    "\n",
    "# 그래서 먼저 데이터를 양성, 음성을 기준으로 데이터를 2개로 나누고\n",
    "\n",
    "## (아플때 - 정상) (-or+ 오차) 를 계산한 값을 리스트로 만든다. (오차는 일단 조건에서 제외)\n",
    "\n",
    "## 제외한 이유 : 오차값을 -오차, 0, 오차로만 해도 3가지인데 이 경우도 \n",
    "\n",
    "## 한 값에 3가지중 랜덤을 돌려도 경우의 수가 너무 많이 나온다.\n",
    "\n",
    "## 그런데 오차값을 랜덤으로 지정하면 경우의 수는 .....\n",
    "\n",
    "## 생각할 변수가 너무 많아 제외하기로 했다\n",
    "\n",
    "# 양성,악성 환자 간의 평균/표준편차을 구해 차이점을 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bef420",
   "metadata": {},
   "source": [
    "## 이번 데이터의 주요 목적 : 악성, 양성을 구별하는것은 생명에 연관된 문제이기 때문에\n",
    "\n",
    "## recall에 더 중점을 두기로 했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12880e16",
   "metadata": {},
   "source": [
    "# 1. 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "762786df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "cancer_temp = cancer_df.copy()\n",
    "cancer_t = cancer_temp.sort_values(by = [cancer_temp.columns[30]])\n",
    "cancer_tar = cancer.target.copy()\n",
    "cancer_tar.sort()\n",
    "np.where(cancer_tar == 1 )\n",
    "cancer_0 = cancer_t[:212]\n",
    "cancer_1 = cancer_t[212:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ada7c",
   "metadata": {},
   "source": [
    "# 2. (아플때 - 정상)값 구해 리스트 or 판다스로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a88aa13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.39</td>\n",
       "      <td>6.95</td>\n",
       "      <td>61.80</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>0.04380</td>\n",
       "      <td>0.38800</td>\n",
       "      <td>0.41180</td>\n",
       "      <td>0.11830</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>0.04019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.52</td>\n",
       "      <td>14.06</td>\n",
       "      <td>12.03</td>\n",
       "      <td>152.5</td>\n",
       "      <td>0.04580</td>\n",
       "      <td>0.69140</td>\n",
       "      <td>0.65070</td>\n",
       "      <td>0.11102</td>\n",
       "      <td>0.2782</td>\n",
       "      <td>0.06918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.31</td>\n",
       "      <td>12.48</td>\n",
       "      <td>20.55</td>\n",
       "      <td>291.5</td>\n",
       "      <td>0.07510</td>\n",
       "      <td>0.33530</td>\n",
       "      <td>0.33400</td>\n",
       "      <td>0.10028</td>\n",
       "      <td>0.1272</td>\n",
       "      <td>0.05113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.22</td>\n",
       "      <td>4.57</td>\n",
       "      <td>18.20</td>\n",
       "      <td>113.3</td>\n",
       "      <td>0.02720</td>\n",
       "      <td>0.26680</td>\n",
       "      <td>0.28920</td>\n",
       "      <td>0.08392</td>\n",
       "      <td>0.1263</td>\n",
       "      <td>0.02029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.62</td>\n",
       "      <td>11.72</td>\n",
       "      <td>20.97</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.05370</td>\n",
       "      <td>0.18550</td>\n",
       "      <td>0.24240</td>\n",
       "      <td>0.10204</td>\n",
       "      <td>0.1246</td>\n",
       "      <td>0.02456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>3.38</td>\n",
       "      <td>10.29</td>\n",
       "      <td>20.55</td>\n",
       "      <td>323.8</td>\n",
       "      <td>0.03162</td>\n",
       "      <td>0.10919</td>\n",
       "      <td>0.23065</td>\n",
       "      <td>0.07351</td>\n",
       "      <td>0.1042</td>\n",
       "      <td>0.01526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>4.83</td>\n",
       "      <td>10.80</td>\n",
       "      <td>28.70</td>\n",
       "      <td>516.2</td>\n",
       "      <td>0.06190</td>\n",
       "      <td>0.22110</td>\n",
       "      <td>0.30620</td>\n",
       "      <td>0.10450</td>\n",
       "      <td>0.1542</td>\n",
       "      <td>0.04064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>4.39</td>\n",
       "      <td>10.75</td>\n",
       "      <td>28.66</td>\n",
       "      <td>453.5</td>\n",
       "      <td>0.04773</td>\n",
       "      <td>0.11510</td>\n",
       "      <td>0.21745</td>\n",
       "      <td>0.10831</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>0.02294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>3.84</td>\n",
       "      <td>4.97</td>\n",
       "      <td>30.00</td>\n",
       "      <td>432.0</td>\n",
       "      <td>0.03800</td>\n",
       "      <td>0.19990</td>\n",
       "      <td>0.27740</td>\n",
       "      <td>0.06699</td>\n",
       "      <td>0.0747</td>\n",
       "      <td>0.02948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>3.35</td>\n",
       "      <td>7.31</td>\n",
       "      <td>20.40</td>\n",
       "      <td>319.1</td>\n",
       "      <td>0.04650</td>\n",
       "      <td>0.20370</td>\n",
       "      <td>0.17414</td>\n",
       "      <td>0.09575</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.04059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0           7.39          6.95           61.80     1018.0          0.04380   \n",
       "1           1.52         14.06           12.03      152.5          0.04580   \n",
       "2           3.31         12.48           20.55      291.5          0.07510   \n",
       "3           1.22          4.57           18.20      113.3          0.02720   \n",
       "4           2.62         11.72           20.97      218.0          0.05370   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "207         3.38         10.29           20.55      323.8          0.03162   \n",
       "208         4.83         10.80           28.70      516.2          0.06190   \n",
       "209         4.39         10.75           28.66      453.5          0.04773   \n",
       "210         3.84          4.97           30.00      432.0          0.03800   \n",
       "211         3.35          7.31           20.40      319.1          0.04650   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.38800         0.41180              0.11830         0.2182   \n",
       "1             0.69140         0.65070              0.11102         0.2782   \n",
       "2             0.33530         0.33400              0.10028         0.1272   \n",
       "3             0.26680         0.28920              0.08392         0.1263   \n",
       "4             0.18550         0.24240              0.10204         0.1246   \n",
       "..                ...             ...                  ...            ...   \n",
       "207           0.10919         0.23065              0.07351         0.1042   \n",
       "208           0.22110         0.30620              0.10450         0.1542   \n",
       "209           0.11510         0.21745              0.10831         0.1443   \n",
       "210           0.19990         0.27740              0.06699         0.0747   \n",
       "211           0.20370         0.17414              0.09575         0.1000   \n",
       "\n",
       "     mean fractal dimension  \n",
       "0                   0.04019  \n",
       "1                   0.06918  \n",
       "2                   0.05113  \n",
       "3                   0.02029  \n",
       "4                   0.02456  \n",
       "..                      ...  \n",
       "207                 0.01526  \n",
       "208                 0.04064  \n",
       "209                 0.02294  \n",
       "210                 0.02948  \n",
       "211                 0.04059  \n",
       "\n",
       "[212 rows x 10 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_0 = []\n",
    "for i in range(len(cancer_0)):\n",
    "    c_0_t = []\n",
    "    for j in range(10):\n",
    "        temp = cancer_0.iloc[i][j+20] - cancer_0.iloc[i][j]\n",
    "        c_0_t.append(temp)\n",
    "    c_0.append(c_0_t)\n",
    "cancer_gap_0 = pd.DataFrame(data = c_0, columns = cancer.feature_names[:10])\n",
    "cancer_gap_0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7496f93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.800</td>\n",
       "      <td>3.07</td>\n",
       "      <td>6.80</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.02470</td>\n",
       "      <td>0.11120</td>\n",
       "      <td>0.14489</td>\n",
       "      <td>0.07113</td>\n",
       "      <td>0.1163</td>\n",
       "      <td>0.02388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.230</td>\n",
       "      <td>7.42</td>\n",
       "      <td>11.22</td>\n",
       "      <td>118.5</td>\n",
       "      <td>0.03563</td>\n",
       "      <td>0.09844</td>\n",
       "      <td>0.06663</td>\n",
       "      <td>0.04664</td>\n",
       "      <td>0.1273</td>\n",
       "      <td>0.01298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.280</td>\n",
       "      <td>2.60</td>\n",
       "      <td>6.36</td>\n",
       "      <td>13.6</td>\n",
       "      <td>0.02357</td>\n",
       "      <td>0.19590</td>\n",
       "      <td>0.37500</td>\n",
       "      <td>0.08709</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.03550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.960</td>\n",
       "      <td>10.06</td>\n",
       "      <td>14.90</td>\n",
       "      <td>129.6</td>\n",
       "      <td>0.04229</td>\n",
       "      <td>0.11217</td>\n",
       "      <td>0.15656</td>\n",
       "      <td>0.06081</td>\n",
       "      <td>0.0949</td>\n",
       "      <td>0.01685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.880</td>\n",
       "      <td>4.49</td>\n",
       "      <td>9.69</td>\n",
       "      <td>76.4</td>\n",
       "      <td>0.04482</td>\n",
       "      <td>0.23492</td>\n",
       "      <td>0.21441</td>\n",
       "      <td>0.08069</td>\n",
       "      <td>0.1595</td>\n",
       "      <td>0.03741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>0.980</td>\n",
       "      <td>4.82</td>\n",
       "      <td>7.01</td>\n",
       "      <td>65.5</td>\n",
       "      <td>0.02530</td>\n",
       "      <td>0.12610</td>\n",
       "      <td>0.16892</td>\n",
       "      <td>0.05733</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>0.01839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>1.030</td>\n",
       "      <td>7.43</td>\n",
       "      <td>6.89</td>\n",
       "      <td>85.3</td>\n",
       "      <td>0.03135</td>\n",
       "      <td>0.11861</td>\n",
       "      <td>0.16020</td>\n",
       "      <td>0.04165</td>\n",
       "      <td>0.1594</td>\n",
       "      <td>0.01293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>1.210</td>\n",
       "      <td>8.20</td>\n",
       "      <td>10.50</td>\n",
       "      <td>100.5</td>\n",
       "      <td>0.02094</td>\n",
       "      <td>0.09041</td>\n",
       "      <td>0.10458</td>\n",
       "      <td>0.04047</td>\n",
       "      <td>0.0811</td>\n",
       "      <td>0.01165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>1.570</td>\n",
       "      <td>4.24</td>\n",
       "      <td>10.46</td>\n",
       "      <td>106.4</td>\n",
       "      <td>0.02547</td>\n",
       "      <td>0.04160</td>\n",
       "      <td>0.04490</td>\n",
       "      <td>0.05527</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.00575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>1.696</td>\n",
       "      <td>5.83</td>\n",
       "      <td>11.24</td>\n",
       "      <td>87.6</td>\n",
       "      <td>0.03733</td>\n",
       "      <td>0.02082</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1284</td>\n",
       "      <td>0.01155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>357 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          0.800          3.07            6.80       73.0          0.02470   \n",
       "1          1.230          7.42           11.22      118.5          0.03563   \n",
       "2          0.280          2.60            6.36       13.6          0.02357   \n",
       "3          1.960         10.06           14.90      129.6          0.04229   \n",
       "4          0.880          4.49            9.69       76.4          0.04482   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "352        0.980          4.82            7.01       65.5          0.02530   \n",
       "353        1.030          7.43            6.89       85.3          0.03135   \n",
       "354        1.210          8.20           10.50      100.5          0.02094   \n",
       "355        1.570          4.24           10.46      106.4          0.02547   \n",
       "356        1.696          5.83           11.24       87.6          0.03733   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.11120         0.14489              0.07113         0.1163   \n",
       "1             0.09844         0.06663              0.04664         0.1273   \n",
       "2             0.19590         0.37500              0.08709         0.0409   \n",
       "3             0.11217         0.15656              0.06081         0.0949   \n",
       "4             0.23492         0.21441              0.08069         0.1595   \n",
       "..                ...             ...                  ...            ...   \n",
       "352           0.12610         0.16892              0.05733         0.0937   \n",
       "353           0.11861         0.16020              0.04165         0.1594   \n",
       "354           0.09041         0.10458              0.04047         0.0811   \n",
       "355           0.04160         0.04490              0.05527         0.1000   \n",
       "356           0.02082         0.00000              0.00000         0.1284   \n",
       "\n",
       "     mean fractal dimension  \n",
       "0                   0.02388  \n",
       "1                   0.01298  \n",
       "2                   0.03550  \n",
       "3                   0.01685  \n",
       "4                   0.03741  \n",
       "..                      ...  \n",
       "352                 0.01839  \n",
       "353                 0.01293  \n",
       "354                 0.01165  \n",
       "355                 0.00575  \n",
       "356                 0.01155  \n",
       "\n",
       "[357 rows x 10 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_1 = []\n",
    "for i in range(len(cancer_1)):\n",
    "    c_1_t = []\n",
    "    for j in range(10):\n",
    "        temp = cancer_1.iloc[i][j+20] - cancer_1.iloc[i][j]\n",
    "        c_1_t.append(temp)\n",
    "    c_1.append(c_1_t)\n",
    "cancer_gap_1 = pd.DataFrame(data = c_1, columns = cancer.feature_names[:10])\n",
    "cancer_gap_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc828ca",
   "metadata": {},
   "source": [
    "# 3. 구한 값으로 평균/표준편차 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b77ee4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min\n",
      "mean radius               0.0\n",
      "mean texture              0.0\n",
      "mean perimeter            0.0\n",
      "mean area                 0.0\n",
      "mean smoothness           0.0\n",
      "mean compactness          0.0\n",
      "mean concavity            0.0\n",
      "mean concave points       0.0\n",
      "mean symmetry             0.0\n",
      "mean fractal dimension    0.0\n",
      "dtype: float64\n",
      "\n",
      "max\n",
      "mean radius                 11.76000\n",
      "mean texture                18.10000\n",
      "mean perimeter              78.30000\n",
      "mean area                 2013.00000\n",
      "mean smoothness              0.09680\n",
      "mean compactness             0.81840\n",
      "mean concavity               0.91500\n",
      "mean concave points          0.18873\n",
      "mean symmetry                0.40410\n",
      "mean fractal dimension       0.12507\n",
      "dtype: float64\n",
      "\n",
      "평균\n",
      "mean radius                 3.671981\n",
      "mean texture                7.713302\n",
      "mean perimeter             26.004953\n",
      "mean area                 443.909906\n",
      "mean smoothness             0.041947\n",
      "mean compactness            0.229636\n",
      "mean concavity              0.289831\n",
      "mean concave points         0.094247\n",
      "mean symmetry               0.130559\n",
      "mean fractal dimension      0.028850\n",
      "dtype: float64\n",
      "\n",
      "표준편차\n",
      "mean radius                 1.821005\n",
      "mean texture                3.005412\n",
      "mean perimeter             12.575927\n",
      "mean area                 303.153809\n",
      "mean smoothness             0.015053\n",
      "mean compactness            0.131868\n",
      "mean concavity              0.135968\n",
      "mean concave points         0.029674\n",
      "mean symmetry               0.058296\n",
      "mean fractal dimension      0.015766\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('min')\n",
    "print(cancer_gap_0.min())\n",
    "print()\n",
    "print('max')\n",
    "print(cancer_gap_0.max())\n",
    "print()\n",
    "print('평균')\n",
    "print(cancer_gap_0.mean())\n",
    "print()\n",
    "print('표준편차')\n",
    "print(cancer_gap_0.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31233ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min\n",
      "mean radius                0.24800\n",
      "mean texture               1.03000\n",
      "mean perimeter             1.52000\n",
      "mean area                 13.60000\n",
      "mean smoothness            0.00167\n",
      "mean compactness           0.00385\n",
      "mean concavity             0.00000\n",
      "mean concave points        0.00000\n",
      "mean symmetry              0.01840\n",
      "mean fractal dimension     0.00112\n",
      "dtype: float64\n",
      "\n",
      "max\n",
      "mean radius                 3.09000\n",
      "mean texture               18.75000\n",
      "mean perimeter             28.13000\n",
      "mean area                 278.90000\n",
      "mean smoothness             0.08231\n",
      "mean compactness            0.45030\n",
      "mean concavity              0.93900\n",
      "mean concave points         0.13125\n",
      "mean symmetry               0.22780\n",
      "mean fractal dimension      0.06599\n",
      "dtype: float64\n",
      "\n",
      "평균\n",
      "mean radius                1.233277\n",
      "mean texture               5.600308\n",
      "mean perimeter             8.930532\n",
      "mean area                 96.109244\n",
      "mean smoothness            0.032482\n",
      "mean compactness           0.102588\n",
      "mean concavity             0.120180\n",
      "mean concave points        0.048727\n",
      "mean symmetry              0.096060\n",
      "mean fractal dimension     0.016575\n",
      "dtype: float64\n",
      "\n",
      "표준편차\n",
      "mean radius                0.450296\n",
      "mean texture               2.414592\n",
      "mean perimeter             3.548752\n",
      "mean area                 42.999351\n",
      "mean smoothness            0.012425\n",
      "mean compactness           0.068205\n",
      "mean concavity             0.104649\n",
      "mean concave points        0.024335\n",
      "mean symmetry              0.032551\n",
      "mean fractal dimension     0.009174\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('min')\n",
    "print(cancer_gap_1.min())\n",
    "print()\n",
    "print('max')\n",
    "print(cancer_gap_1.max())\n",
    "print()\n",
    "print('평균')\n",
    "print(cancer_gap_1.mean())\n",
    "print()\n",
    "print('표준편차')\n",
    "print(cancer_gap_1.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e681fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>12.83</td>\n",
       "      <td>22.33</td>\n",
       "      <td>85.26</td>\n",
       "      <td>503.2</td>\n",
       "      <td>0.10880</td>\n",
       "      <td>0.17990</td>\n",
       "      <td>0.16950</td>\n",
       "      <td>0.06861</td>\n",
       "      <td>0.2123</td>\n",
       "      <td>0.07254</td>\n",
       "      <td>...</td>\n",
       "      <td>30.15</td>\n",
       "      <td>105.30</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.5343</td>\n",
       "      <td>0.6282</td>\n",
       "      <td>0.19770</td>\n",
       "      <td>0.3407</td>\n",
       "      <td>0.12430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>12.77</td>\n",
       "      <td>22.47</td>\n",
       "      <td>81.72</td>\n",
       "      <td>506.3</td>\n",
       "      <td>0.09055</td>\n",
       "      <td>0.05761</td>\n",
       "      <td>0.04711</td>\n",
       "      <td>0.02704</td>\n",
       "      <td>0.1585</td>\n",
       "      <td>0.06065</td>\n",
       "      <td>...</td>\n",
       "      <td>33.37</td>\n",
       "      <td>92.04</td>\n",
       "      <td>653.6</td>\n",
       "      <td>0.1419</td>\n",
       "      <td>0.1523</td>\n",
       "      <td>0.2177</td>\n",
       "      <td>0.09331</td>\n",
       "      <td>0.2829</td>\n",
       "      <td>0.08067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.00</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.20600</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>13.11</td>\n",
       "      <td>15.56</td>\n",
       "      <td>87.21</td>\n",
       "      <td>530.2</td>\n",
       "      <td>0.13980</td>\n",
       "      <td>0.17650</td>\n",
       "      <td>0.20710</td>\n",
       "      <td>0.09601</td>\n",
       "      <td>0.1925</td>\n",
       "      <td>0.07692</td>\n",
       "      <td>...</td>\n",
       "      <td>22.40</td>\n",
       "      <td>106.40</td>\n",
       "      <td>827.2</td>\n",
       "      <td>0.1862</td>\n",
       "      <td>0.4099</td>\n",
       "      <td>0.6376</td>\n",
       "      <td>0.19860</td>\n",
       "      <td>0.3147</td>\n",
       "      <td>0.14050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>13.17</td>\n",
       "      <td>21.81</td>\n",
       "      <td>85.42</td>\n",
       "      <td>531.5</td>\n",
       "      <td>0.09714</td>\n",
       "      <td>0.10470</td>\n",
       "      <td>0.08259</td>\n",
       "      <td>0.05252</td>\n",
       "      <td>0.1746</td>\n",
       "      <td>0.06177</td>\n",
       "      <td>...</td>\n",
       "      <td>29.89</td>\n",
       "      <td>105.50</td>\n",
       "      <td>740.7</td>\n",
       "      <td>0.1503</td>\n",
       "      <td>0.3904</td>\n",
       "      <td>0.3728</td>\n",
       "      <td>0.16070</td>\n",
       "      <td>0.3693</td>\n",
       "      <td>0.09618</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>21.75</td>\n",
       "      <td>20.99</td>\n",
       "      <td>147.30</td>\n",
       "      <td>1491.0</td>\n",
       "      <td>0.09401</td>\n",
       "      <td>0.19610</td>\n",
       "      <td>0.21950</td>\n",
       "      <td>0.10880</td>\n",
       "      <td>0.1721</td>\n",
       "      <td>0.06194</td>\n",
       "      <td>...</td>\n",
       "      <td>28.18</td>\n",
       "      <td>195.90</td>\n",
       "      <td>2384.0</td>\n",
       "      <td>0.1272</td>\n",
       "      <td>0.4725</td>\n",
       "      <td>0.5807</td>\n",
       "      <td>0.18410</td>\n",
       "      <td>0.2833</td>\n",
       "      <td>0.08858</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>22.27</td>\n",
       "      <td>19.67</td>\n",
       "      <td>152.80</td>\n",
       "      <td>1509.0</td>\n",
       "      <td>0.13260</td>\n",
       "      <td>0.27680</td>\n",
       "      <td>0.42640</td>\n",
       "      <td>0.18230</td>\n",
       "      <td>0.2556</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>...</td>\n",
       "      <td>28.01</td>\n",
       "      <td>206.80</td>\n",
       "      <td>2360.0</td>\n",
       "      <td>0.1701</td>\n",
       "      <td>0.6997</td>\n",
       "      <td>0.9608</td>\n",
       "      <td>0.29100</td>\n",
       "      <td>0.4055</td>\n",
       "      <td>0.09789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>21.71</td>\n",
       "      <td>17.25</td>\n",
       "      <td>140.90</td>\n",
       "      <td>1546.0</td>\n",
       "      <td>0.09384</td>\n",
       "      <td>0.08562</td>\n",
       "      <td>0.11680</td>\n",
       "      <td>0.08465</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>0.05054</td>\n",
       "      <td>...</td>\n",
       "      <td>26.44</td>\n",
       "      <td>199.50</td>\n",
       "      <td>3143.0</td>\n",
       "      <td>0.1363</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2861</td>\n",
       "      <td>0.18200</td>\n",
       "      <td>0.2510</td>\n",
       "      <td>0.06494</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>23.21</td>\n",
       "      <td>26.97</td>\n",
       "      <td>153.50</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>0.09509</td>\n",
       "      <td>0.16820</td>\n",
       "      <td>0.19500</td>\n",
       "      <td>0.12370</td>\n",
       "      <td>0.1909</td>\n",
       "      <td>0.06309</td>\n",
       "      <td>...</td>\n",
       "      <td>34.51</td>\n",
       "      <td>206.00</td>\n",
       "      <td>2944.0</td>\n",
       "      <td>0.1481</td>\n",
       "      <td>0.4126</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.25930</td>\n",
       "      <td>0.3103</td>\n",
       "      <td>0.08677</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>23.09</td>\n",
       "      <td>19.83</td>\n",
       "      <td>152.10</td>\n",
       "      <td>1682.0</td>\n",
       "      <td>0.09342</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0.16760</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.05484</td>\n",
       "      <td>...</td>\n",
       "      <td>23.87</td>\n",
       "      <td>211.50</td>\n",
       "      <td>2782.0</td>\n",
       "      <td>0.1199</td>\n",
       "      <td>0.3625</td>\n",
       "      <td>0.3794</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.2908</td>\n",
       "      <td>0.07277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "229        12.83         22.33           85.26      503.2          0.10880   \n",
       "135        12.77         22.47           81.72      506.3          0.09055   \n",
       "8          13.00         21.82           87.50      519.8          0.12730   \n",
       "105        13.11         15.56           87.21      530.2          0.13980   \n",
       "44         13.17         21.81           85.42      531.5          0.09714   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "272        21.75         20.99          147.30     1491.0          0.09401   \n",
       "108        22.27         19.67          152.80     1509.0          0.13260   \n",
       "368        21.71         17.25          140.90     1546.0          0.09384   \n",
       "236        23.21         26.97          153.50     1670.0          0.09509   \n",
       "503        23.09         19.83          152.10     1682.0          0.09342   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "229           0.17990         0.16950              0.06861         0.2123   \n",
       "135           0.05761         0.04711              0.02704         0.1585   \n",
       "8             0.19320         0.18590              0.09353         0.2350   \n",
       "105           0.17650         0.20710              0.09601         0.1925   \n",
       "44            0.10470         0.08259              0.05252         0.1746   \n",
       "..                ...             ...                  ...            ...   \n",
       "272           0.19610         0.21950              0.10880         0.1721   \n",
       "108           0.27680         0.42640              0.18230         0.2556   \n",
       "368           0.08562         0.11680              0.08465         0.1717   \n",
       "236           0.16820         0.19500              0.12370         0.1909   \n",
       "503           0.12750         0.16760              0.10030         0.1505   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "229                 0.07254  ...          30.15           105.30       706.0   \n",
       "135                 0.06065  ...          33.37            92.04       653.6   \n",
       "8                   0.07389  ...          30.73           106.20       739.3   \n",
       "105                 0.07692  ...          22.40           106.40       827.2   \n",
       "44                  0.06177  ...          29.89           105.50       740.7   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "272                 0.06194  ...          28.18           195.90      2384.0   \n",
       "108                 0.07039  ...          28.01           206.80      2360.0   \n",
       "368                 0.05054  ...          26.44           199.50      3143.0   \n",
       "236                 0.06309  ...          34.51           206.00      2944.0   \n",
       "503                 0.05484  ...          23.87           211.50      2782.0   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "229            0.1777             0.5343           0.6282   \n",
       "135            0.1419             0.1523           0.2177   \n",
       "8              0.1703             0.5401           0.5390   \n",
       "105            0.1862             0.4099           0.6376   \n",
       "44             0.1503             0.3904           0.3728   \n",
       "..                ...                ...              ...   \n",
       "272            0.1272             0.4725           0.5807   \n",
       "108            0.1701             0.6997           0.9608   \n",
       "368            0.1363             0.1628           0.2861   \n",
       "236            0.1481             0.4126           0.5820   \n",
       "503            0.1199             0.3625           0.3794   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  label  \n",
       "229               0.19770          0.3407                  0.12430      0  \n",
       "135               0.09331          0.2829                  0.08067      0  \n",
       "8                 0.20600          0.4378                  0.10720      0  \n",
       "105               0.19860          0.3147                  0.14050      0  \n",
       "44                0.16070          0.3693                  0.09618      0  \n",
       "..                    ...             ...                      ...    ...  \n",
       "272               0.18410          0.2833                  0.08858      0  \n",
       "108               0.29100          0.4055                  0.09789      0  \n",
       "368               0.18200          0.2510                  0.06494      0  \n",
       "236               0.25930          0.3103                  0.08677      0  \n",
       "503               0.22640          0.2908                  0.07277      0  \n",
       "\n",
       "[192 rows x 31 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cacp_0 = cancer_0.copy()\n",
    "cancer_sort_0 = cacp_0.sort_values(by = [cancer_0.columns[3], cancer_0.columns[2], cancer_0.columns[1]])\n",
    "cancer_cut_0 = cancer_sort_0[10:202]\n",
    "cancer_cut_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2d8a9e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>9.000</td>\n",
       "      <td>14.40</td>\n",
       "      <td>56.36</td>\n",
       "      <td>246.3</td>\n",
       "      <td>0.07005</td>\n",
       "      <td>0.03116</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.003472</td>\n",
       "      <td>0.1788</td>\n",
       "      <td>0.06833</td>\n",
       "      <td>...</td>\n",
       "      <td>20.07</td>\n",
       "      <td>60.90</td>\n",
       "      <td>285.5</td>\n",
       "      <td>0.09861</td>\n",
       "      <td>0.05232</td>\n",
       "      <td>0.01472</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.07804</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>9.268</td>\n",
       "      <td>12.87</td>\n",
       "      <td>61.49</td>\n",
       "      <td>248.7</td>\n",
       "      <td>0.16340</td>\n",
       "      <td>0.22390</td>\n",
       "      <td>0.097300</td>\n",
       "      <td>0.052520</td>\n",
       "      <td>0.2378</td>\n",
       "      <td>0.09502</td>\n",
       "      <td>...</td>\n",
       "      <td>16.38</td>\n",
       "      <td>69.05</td>\n",
       "      <td>300.2</td>\n",
       "      <td>0.19020</td>\n",
       "      <td>0.34410</td>\n",
       "      <td>0.20990</td>\n",
       "      <td>0.10250</td>\n",
       "      <td>0.3038</td>\n",
       "      <td>0.12520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>9.029</td>\n",
       "      <td>17.33</td>\n",
       "      <td>58.79</td>\n",
       "      <td>250.5</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.14130</td>\n",
       "      <td>0.313000</td>\n",
       "      <td>0.043750</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.08046</td>\n",
       "      <td>...</td>\n",
       "      <td>22.65</td>\n",
       "      <td>65.50</td>\n",
       "      <td>324.7</td>\n",
       "      <td>0.14820</td>\n",
       "      <td>0.43650</td>\n",
       "      <td>1.25200</td>\n",
       "      <td>0.17500</td>\n",
       "      <td>0.4228</td>\n",
       "      <td>0.11750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>9.295</td>\n",
       "      <td>13.90</td>\n",
       "      <td>59.96</td>\n",
       "      <td>257.8</td>\n",
       "      <td>0.13710</td>\n",
       "      <td>0.12250</td>\n",
       "      <td>0.033320</td>\n",
       "      <td>0.024210</td>\n",
       "      <td>0.2197</td>\n",
       "      <td>0.07696</td>\n",
       "      <td>...</td>\n",
       "      <td>17.84</td>\n",
       "      <td>67.84</td>\n",
       "      <td>326.6</td>\n",
       "      <td>0.18500</td>\n",
       "      <td>0.20970</td>\n",
       "      <td>0.09996</td>\n",
       "      <td>0.07262</td>\n",
       "      <td>0.3681</td>\n",
       "      <td>0.08982</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>9.173</td>\n",
       "      <td>13.86</td>\n",
       "      <td>59.20</td>\n",
       "      <td>260.9</td>\n",
       "      <td>0.07721</td>\n",
       "      <td>0.08751</td>\n",
       "      <td>0.059880</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.06963</td>\n",
       "      <td>...</td>\n",
       "      <td>19.23</td>\n",
       "      <td>65.59</td>\n",
       "      <td>310.1</td>\n",
       "      <td>0.09836</td>\n",
       "      <td>0.16780</td>\n",
       "      <td>0.13970</td>\n",
       "      <td>0.05087</td>\n",
       "      <td>0.3282</td>\n",
       "      <td>0.08490</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>15.100</td>\n",
       "      <td>16.39</td>\n",
       "      <td>99.58</td>\n",
       "      <td>674.5</td>\n",
       "      <td>0.11500</td>\n",
       "      <td>0.18070</td>\n",
       "      <td>0.113800</td>\n",
       "      <td>0.085340</td>\n",
       "      <td>0.2001</td>\n",
       "      <td>0.06467</td>\n",
       "      <td>...</td>\n",
       "      <td>18.33</td>\n",
       "      <td>105.90</td>\n",
       "      <td>762.6</td>\n",
       "      <td>0.13860</td>\n",
       "      <td>0.28830</td>\n",
       "      <td>0.19600</td>\n",
       "      <td>0.14230</td>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.07779</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>14.800</td>\n",
       "      <td>17.66</td>\n",
       "      <td>95.88</td>\n",
       "      <td>674.8</td>\n",
       "      <td>0.09179</td>\n",
       "      <td>0.08890</td>\n",
       "      <td>0.040690</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.1893</td>\n",
       "      <td>0.05886</td>\n",
       "      <td>...</td>\n",
       "      <td>22.74</td>\n",
       "      <td>105.90</td>\n",
       "      <td>829.5</td>\n",
       "      <td>0.12260</td>\n",
       "      <td>0.18810</td>\n",
       "      <td>0.20600</td>\n",
       "      <td>0.08308</td>\n",
       "      <td>0.3600</td>\n",
       "      <td>0.07285</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>14.810</td>\n",
       "      <td>14.70</td>\n",
       "      <td>94.66</td>\n",
       "      <td>680.7</td>\n",
       "      <td>0.08472</td>\n",
       "      <td>0.05016</td>\n",
       "      <td>0.034160</td>\n",
       "      <td>0.025410</td>\n",
       "      <td>0.1659</td>\n",
       "      <td>0.05348</td>\n",
       "      <td>...</td>\n",
       "      <td>17.58</td>\n",
       "      <td>101.70</td>\n",
       "      <td>760.2</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.10110</td>\n",
       "      <td>0.11010</td>\n",
       "      <td>0.07955</td>\n",
       "      <td>0.2334</td>\n",
       "      <td>0.06142</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>14.870</td>\n",
       "      <td>20.21</td>\n",
       "      <td>96.12</td>\n",
       "      <td>680.9</td>\n",
       "      <td>0.09587</td>\n",
       "      <td>0.08345</td>\n",
       "      <td>0.068240</td>\n",
       "      <td>0.049510</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.05748</td>\n",
       "      <td>...</td>\n",
       "      <td>28.48</td>\n",
       "      <td>103.90</td>\n",
       "      <td>783.6</td>\n",
       "      <td>0.12160</td>\n",
       "      <td>0.13880</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.10170</td>\n",
       "      <td>0.2369</td>\n",
       "      <td>0.06599</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>15.000</td>\n",
       "      <td>15.51</td>\n",
       "      <td>97.45</td>\n",
       "      <td>684.5</td>\n",
       "      <td>0.08371</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.065050</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.1881</td>\n",
       "      <td>0.05907</td>\n",
       "      <td>...</td>\n",
       "      <td>19.31</td>\n",
       "      <td>114.20</td>\n",
       "      <td>808.2</td>\n",
       "      <td>0.11360</td>\n",
       "      <td>0.36270</td>\n",
       "      <td>0.34020</td>\n",
       "      <td>0.13790</td>\n",
       "      <td>0.2954</td>\n",
       "      <td>0.08362</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>323 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "307        9.000         14.40           56.36      246.3          0.07005   \n",
       "504        9.268         12.87           61.49      248.7          0.16340   \n",
       "68         9.029         17.33           58.79      250.5          0.10660   \n",
       "520        9.295         13.90           59.96      257.8          0.13710   \n",
       "63         9.173         13.86           59.20      260.9          0.07721   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "128       15.100         16.39           99.58      674.5          0.11500   \n",
       "447       14.800         17.66           95.88      674.8          0.09179   \n",
       "511       14.810         14.70           94.66      680.7          0.08472   \n",
       "495       14.870         20.21           96.12      680.9          0.09587   \n",
       "227       15.000         15.51           97.45      684.5          0.08371   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "307           0.03116        0.003681             0.003472         0.1788   \n",
       "504           0.22390        0.097300             0.052520         0.2378   \n",
       "68            0.14130        0.313000             0.043750         0.2111   \n",
       "520           0.12250        0.033320             0.024210         0.2197   \n",
       "63            0.08751        0.059880             0.021800         0.2341   \n",
       "..                ...             ...                  ...            ...   \n",
       "128           0.18070        0.113800             0.085340         0.2001   \n",
       "447           0.08890        0.040690             0.022600         0.1893   \n",
       "511           0.05016        0.034160             0.025410         0.1659   \n",
       "495           0.08345        0.068240             0.049510         0.1487   \n",
       "227           0.10960        0.065050             0.037800         0.1881   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "307                 0.06833  ...          20.07            60.90       285.5   \n",
       "504                 0.09502  ...          16.38            69.05       300.2   \n",
       "68                  0.08046  ...          22.65            65.50       324.7   \n",
       "520                 0.07696  ...          17.84            67.84       326.6   \n",
       "63                  0.06963  ...          19.23            65.59       310.1   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "128                 0.06467  ...          18.33           105.90       762.6   \n",
       "447                 0.05886  ...          22.74           105.90       829.5   \n",
       "511                 0.05348  ...          17.58           101.70       760.2   \n",
       "495                 0.05748  ...          28.48           103.90       783.6   \n",
       "227                 0.05907  ...          19.31           114.20       808.2   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "307           0.09861            0.05232          0.01472   \n",
       "504           0.19020            0.34410          0.20990   \n",
       "68            0.14820            0.43650          1.25200   \n",
       "520           0.18500            0.20970          0.09996   \n",
       "63            0.09836            0.16780          0.13970   \n",
       "..                ...                ...              ...   \n",
       "128           0.13860            0.28830          0.19600   \n",
       "447           0.12260            0.18810          0.20600   \n",
       "511           0.11390            0.10110          0.11010   \n",
       "495           0.12160            0.13880          0.17000   \n",
       "227           0.11360            0.36270          0.34020   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  label  \n",
       "307               0.01389          0.2991                  0.07804      1  \n",
       "504               0.10250          0.3038                  0.12520      1  \n",
       "68                0.17500          0.4228                  0.11750      1  \n",
       "520               0.07262          0.3681                  0.08982      1  \n",
       "63                0.05087          0.3282                  0.08490      1  \n",
       "..                    ...             ...                      ...    ...  \n",
       "128               0.14230          0.2590                  0.07779      1  \n",
       "447               0.08308          0.3600                  0.07285      1  \n",
       "511               0.07955          0.2334                  0.06142      1  \n",
       "495               0.10170          0.2369                  0.06599      1  \n",
       "227               0.13790          0.2954                  0.08362      1  \n",
       "\n",
       "[323 rows x 31 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cacp_1 = cancer_1.copy()\n",
    "cancer_sort_1 = cacp_1.sort_values(by = [cancer_1.columns[3], cancer_1.columns[2], cancer_1.columns[1]])\n",
    "cancer_cut_1 = cancer_sort_1[17:340]\n",
    "cancer_cut_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2dcbde97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>12.83</td>\n",
       "      <td>22.33</td>\n",
       "      <td>85.26</td>\n",
       "      <td>503.2</td>\n",
       "      <td>0.10880</td>\n",
       "      <td>0.17990</td>\n",
       "      <td>0.16950</td>\n",
       "      <td>0.06861</td>\n",
       "      <td>0.2123</td>\n",
       "      <td>0.07254</td>\n",
       "      <td>...</td>\n",
       "      <td>30.15</td>\n",
       "      <td>105.30</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.5343</td>\n",
       "      <td>0.6282</td>\n",
       "      <td>0.19770</td>\n",
       "      <td>0.3407</td>\n",
       "      <td>0.12430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>12.77</td>\n",
       "      <td>22.47</td>\n",
       "      <td>81.72</td>\n",
       "      <td>506.3</td>\n",
       "      <td>0.09055</td>\n",
       "      <td>0.05761</td>\n",
       "      <td>0.04711</td>\n",
       "      <td>0.02704</td>\n",
       "      <td>0.1585</td>\n",
       "      <td>0.06065</td>\n",
       "      <td>...</td>\n",
       "      <td>33.37</td>\n",
       "      <td>92.04</td>\n",
       "      <td>653.6</td>\n",
       "      <td>0.1419</td>\n",
       "      <td>0.1523</td>\n",
       "      <td>0.2177</td>\n",
       "      <td>0.09331</td>\n",
       "      <td>0.2829</td>\n",
       "      <td>0.08067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.00</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.20600</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>13.11</td>\n",
       "      <td>15.56</td>\n",
       "      <td>87.21</td>\n",
       "      <td>530.2</td>\n",
       "      <td>0.13980</td>\n",
       "      <td>0.17650</td>\n",
       "      <td>0.20710</td>\n",
       "      <td>0.09601</td>\n",
       "      <td>0.1925</td>\n",
       "      <td>0.07692</td>\n",
       "      <td>...</td>\n",
       "      <td>22.40</td>\n",
       "      <td>106.40</td>\n",
       "      <td>827.2</td>\n",
       "      <td>0.1862</td>\n",
       "      <td>0.4099</td>\n",
       "      <td>0.6376</td>\n",
       "      <td>0.19860</td>\n",
       "      <td>0.3147</td>\n",
       "      <td>0.14050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>13.17</td>\n",
       "      <td>21.81</td>\n",
       "      <td>85.42</td>\n",
       "      <td>531.5</td>\n",
       "      <td>0.09714</td>\n",
       "      <td>0.10470</td>\n",
       "      <td>0.08259</td>\n",
       "      <td>0.05252</td>\n",
       "      <td>0.1746</td>\n",
       "      <td>0.06177</td>\n",
       "      <td>...</td>\n",
       "      <td>29.89</td>\n",
       "      <td>105.50</td>\n",
       "      <td>740.7</td>\n",
       "      <td>0.1503</td>\n",
       "      <td>0.3904</td>\n",
       "      <td>0.3728</td>\n",
       "      <td>0.16070</td>\n",
       "      <td>0.3693</td>\n",
       "      <td>0.09618</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>15.10</td>\n",
       "      <td>16.39</td>\n",
       "      <td>99.58</td>\n",
       "      <td>674.5</td>\n",
       "      <td>0.11500</td>\n",
       "      <td>0.18070</td>\n",
       "      <td>0.11380</td>\n",
       "      <td>0.08534</td>\n",
       "      <td>0.2001</td>\n",
       "      <td>0.06467</td>\n",
       "      <td>...</td>\n",
       "      <td>18.33</td>\n",
       "      <td>105.90</td>\n",
       "      <td>762.6</td>\n",
       "      <td>0.1386</td>\n",
       "      <td>0.2883</td>\n",
       "      <td>0.1960</td>\n",
       "      <td>0.14230</td>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.07779</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>14.80</td>\n",
       "      <td>17.66</td>\n",
       "      <td>95.88</td>\n",
       "      <td>674.8</td>\n",
       "      <td>0.09179</td>\n",
       "      <td>0.08890</td>\n",
       "      <td>0.04069</td>\n",
       "      <td>0.02260</td>\n",
       "      <td>0.1893</td>\n",
       "      <td>0.05886</td>\n",
       "      <td>...</td>\n",
       "      <td>22.74</td>\n",
       "      <td>105.90</td>\n",
       "      <td>829.5</td>\n",
       "      <td>0.1226</td>\n",
       "      <td>0.1881</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.08308</td>\n",
       "      <td>0.3600</td>\n",
       "      <td>0.07285</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>14.81</td>\n",
       "      <td>14.70</td>\n",
       "      <td>94.66</td>\n",
       "      <td>680.7</td>\n",
       "      <td>0.08472</td>\n",
       "      <td>0.05016</td>\n",
       "      <td>0.03416</td>\n",
       "      <td>0.02541</td>\n",
       "      <td>0.1659</td>\n",
       "      <td>0.05348</td>\n",
       "      <td>...</td>\n",
       "      <td>17.58</td>\n",
       "      <td>101.70</td>\n",
       "      <td>760.2</td>\n",
       "      <td>0.1139</td>\n",
       "      <td>0.1011</td>\n",
       "      <td>0.1101</td>\n",
       "      <td>0.07955</td>\n",
       "      <td>0.2334</td>\n",
       "      <td>0.06142</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>14.87</td>\n",
       "      <td>20.21</td>\n",
       "      <td>96.12</td>\n",
       "      <td>680.9</td>\n",
       "      <td>0.09587</td>\n",
       "      <td>0.08345</td>\n",
       "      <td>0.06824</td>\n",
       "      <td>0.04951</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.05748</td>\n",
       "      <td>...</td>\n",
       "      <td>28.48</td>\n",
       "      <td>103.90</td>\n",
       "      <td>783.6</td>\n",
       "      <td>0.1216</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.10170</td>\n",
       "      <td>0.2369</td>\n",
       "      <td>0.06599</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>15.00</td>\n",
       "      <td>15.51</td>\n",
       "      <td>97.45</td>\n",
       "      <td>684.5</td>\n",
       "      <td>0.08371</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.06505</td>\n",
       "      <td>0.03780</td>\n",
       "      <td>0.1881</td>\n",
       "      <td>0.05907</td>\n",
       "      <td>...</td>\n",
       "      <td>19.31</td>\n",
       "      <td>114.20</td>\n",
       "      <td>808.2</td>\n",
       "      <td>0.1136</td>\n",
       "      <td>0.3627</td>\n",
       "      <td>0.3402</td>\n",
       "      <td>0.13790</td>\n",
       "      <td>0.2954</td>\n",
       "      <td>0.08362</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>515 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "229        12.83         22.33           85.26      503.2          0.10880   \n",
       "135        12.77         22.47           81.72      506.3          0.09055   \n",
       "8          13.00         21.82           87.50      519.8          0.12730   \n",
       "105        13.11         15.56           87.21      530.2          0.13980   \n",
       "44         13.17         21.81           85.42      531.5          0.09714   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "128        15.10         16.39           99.58      674.5          0.11500   \n",
       "447        14.80         17.66           95.88      674.8          0.09179   \n",
       "511        14.81         14.70           94.66      680.7          0.08472   \n",
       "495        14.87         20.21           96.12      680.9          0.09587   \n",
       "227        15.00         15.51           97.45      684.5          0.08371   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "229           0.17990         0.16950              0.06861         0.2123   \n",
       "135           0.05761         0.04711              0.02704         0.1585   \n",
       "8             0.19320         0.18590              0.09353         0.2350   \n",
       "105           0.17650         0.20710              0.09601         0.1925   \n",
       "44            0.10470         0.08259              0.05252         0.1746   \n",
       "..                ...             ...                  ...            ...   \n",
       "128           0.18070         0.11380              0.08534         0.2001   \n",
       "447           0.08890         0.04069              0.02260         0.1893   \n",
       "511           0.05016         0.03416              0.02541         0.1659   \n",
       "495           0.08345         0.06824              0.04951         0.1487   \n",
       "227           0.10960         0.06505              0.03780         0.1881   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "229                 0.07254  ...          30.15           105.30       706.0   \n",
       "135                 0.06065  ...          33.37            92.04       653.6   \n",
       "8                   0.07389  ...          30.73           106.20       739.3   \n",
       "105                 0.07692  ...          22.40           106.40       827.2   \n",
       "44                  0.06177  ...          29.89           105.50       740.7   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "128                 0.06467  ...          18.33           105.90       762.6   \n",
       "447                 0.05886  ...          22.74           105.90       829.5   \n",
       "511                 0.05348  ...          17.58           101.70       760.2   \n",
       "495                 0.05748  ...          28.48           103.90       783.6   \n",
       "227                 0.05907  ...          19.31           114.20       808.2   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "229            0.1777             0.5343           0.6282   \n",
       "135            0.1419             0.1523           0.2177   \n",
       "8              0.1703             0.5401           0.5390   \n",
       "105            0.1862             0.4099           0.6376   \n",
       "44             0.1503             0.3904           0.3728   \n",
       "..                ...                ...              ...   \n",
       "128            0.1386             0.2883           0.1960   \n",
       "447            0.1226             0.1881           0.2060   \n",
       "511            0.1139             0.1011           0.1101   \n",
       "495            0.1216             0.1388           0.1700   \n",
       "227            0.1136             0.3627           0.3402   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  label  \n",
       "229               0.19770          0.3407                  0.12430      0  \n",
       "135               0.09331          0.2829                  0.08067      0  \n",
       "8                 0.20600          0.4378                  0.10720      0  \n",
       "105               0.19860          0.3147                  0.14050      0  \n",
       "44                0.16070          0.3693                  0.09618      0  \n",
       "..                    ...             ...                      ...    ...  \n",
       "128               0.14230          0.2590                  0.07779      1  \n",
       "447               0.08308          0.3600                  0.07285      1  \n",
       "511               0.07955          0.2334                  0.06142      1  \n",
       "495               0.10170          0.2369                  0.06599      1  \n",
       "227               0.13790          0.2954                  0.08362      1  \n",
       "\n",
       "[515 rows x 31 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_sum = pd.concat([cancer_cut_0, cancer_cut_1])\n",
    "cancer_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6573d9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cancer_target = cancer.target.copy()\n",
    "cancer_target.sort()\n",
    "cancer_cut_target = cancer_target[20:535]\n",
    "len(cancer_cut_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fcfcb3",
   "metadata": {},
   "source": [
    "# 각 데이터로 머신러닝 돌리기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b700a72",
   "metadata": {},
   "source": [
    "# 주어진 데이터 수정없이 돌렸을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f4395957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.88      0.91        65\n",
      "           1       0.93      0.97      0.95       106\n",
      "\n",
      "    accuracy                           0.94       171\n",
      "   macro avg       0.94      0.92      0.93       171\n",
      "weighted avg       0.94      0.94      0.94       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=random.randrange(30,70))\n",
    "random_forest = RandomForestClassifier(random_state=random.randrange(40,80))\n",
    "svm_model = svm.SVC()\n",
    "sgd_model = SGDClassifier()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, \n",
    "                                                    cancer.target, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=random.randrange(20,100))\n",
    "\n",
    "decision_tree.fit(X_train, y_train)\n",
    "dec_pred = decision_tree.predict(X_test)\n",
    "print(classification_report(y_test, dec_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ccdb56f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        65\n",
      "           1       0.96      1.00      0.98       106\n",
      "\n",
      "    accuracy                           0.98       171\n",
      "   macro avg       0.98      0.97      0.97       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest.fit(X_train, y_train)\n",
    "ran_pred = random_forest.predict(X_test)\n",
    "print(classification_report(y_test, ran_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "895901c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.78      0.87        65\n",
      "           1       0.88      0.99      0.93       106\n",
      "\n",
      "    accuracy                           0.91       171\n",
      "   macro avg       0.93      0.89      0.90       171\n",
      "weighted avg       0.92      0.91      0.91       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_model.fit(X_train, y_train)\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "print(classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8df06156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.77      0.86        65\n",
      "           1       0.88      0.99      0.93       106\n",
      "\n",
      "    accuracy                           0.91       171\n",
      "   macro avg       0.93      0.88      0.90       171\n",
      "weighted avg       0.92      0.91      0.90       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd_model.fit(X_train, y_train)\n",
    "sgd_pred = sgd_model.predict(X_test)\n",
    "print(classification_report(y_test, sgd_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e397e071",
   "metadata": {},
   "source": [
    "# 데이터 전처리 후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "26a39d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        58\n",
      "           1       1.00      1.00      1.00        97\n",
      "\n",
      "    accuracy                           1.00       155\n",
      "   macro avg       1.00      1.00      1.00       155\n",
      "weighted avg       1.00      1.00      1.00       155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(random_state=random.randrange(30,100))\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer_sum, \n",
    "                                                    cancer_cut_target, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=random.randrange(30,200))\n",
    "\n",
    "decision_tree.fit(X_train, y_train)\n",
    "dec_pred = decision_tree.predict(X_test)\n",
    "print(classification_report(y_test, dec_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "88418af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        58\n",
      "           1       1.00      1.00      1.00        97\n",
      "\n",
      "    accuracy                           1.00       155\n",
      "   macro avg       1.00      1.00      1.00       155\n",
      "weighted avg       1.00      1.00      1.00       155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(random_state=random.randrange(40,100))\n",
    "random_forest.fit(X_train, y_train)\n",
    "ran_pred = random_forest.predict(X_test)\n",
    "print(classification_report(y_test, ran_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "46a3eeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.94        56\n",
      "           1       0.95      0.98      0.97        99\n",
      "\n",
      "    accuracy                           0.95       155\n",
      "   macro avg       0.96      0.95      0.95       155\n",
      "weighted avg       0.96      0.95      0.95       155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_model.fit(X_train, y_train)\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "print(classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cc0a9eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.87        56\n",
      "           1       0.89      0.99      0.94        99\n",
      "\n",
      "    accuracy                           0.92       155\n",
      "   macro avg       0.93      0.89      0.90       155\n",
      "weighted avg       0.92      0.92      0.91       155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd_model.fit(X_train, y_train)\n",
    "sgd_pred = sgd_model.predict(X_test)\n",
    "print(classification_report(y_test, sgd_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf27058",
   "metadata": {},
   "source": [
    "# 돌린 결과: 전처리 전 vs 후 성능 비교는 정확도, recall 둘다 후가 높았고\n",
    "\n",
    "# 모델 성능은 의사결정 == 랜덤 포레스트 > svm (== or >) sgd 이었다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
